{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1be82-fb73-48ed-a226-ebadc49a255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do two steps: \n",
    "# 1: Export person bounding boxes with YOLO: \n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.15\n",
    "\n",
    "# === Paths ===\n",
    "video_folder = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO\")\n",
    "output_base = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO/DLC\")\n",
    "\n",
    "if output_base.exists():\n",
    "    shutil.rmtree(output_base)\n",
    "output_base.mkdir(parents=True)\n",
    "\n",
    "video_files = list(video_folder.glob(\"*.mp4\")) + list(video_folder.glob(\"*.MP4\")) + \\\n",
    "              list(video_folder.glob(\"*.Mp4\")) + list(video_folder.glob(\"*.mP4\"))\n",
    "\n",
    "print(f\"üé• Found {len(video_files)} video(s) to process.\")\n",
    "if not video_files:\n",
    "    raise FileNotFoundError(\"No video files found!\")\n",
    "\n",
    "# === Load YOLOv11 model\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Using device: {device}\")\n",
    "model = YOLO(\"yolo11l.pt\")\n",
    "\n",
    "# === Detection Loop ===\n",
    "for i, video_path in enumerate(tqdm(video_files, desc=\"YOLO Detection\", position=0)):\n",
    "    print(f\"\\nüìπ [{i+1}/{len(video_files)}] Processing: {video_path.name}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ö†Ô∏è Could not open video: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_name = video_path.stem\n",
    "    frame_output_dir = output_base / video_name / \"frames\"\n",
    "    bbox_output_dir = output_base / video_name / \"bboxes\"\n",
    "    frame_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    bbox_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"Processing frames\", leave=False, position=1) as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                break\n",
    "\n",
    "            # Save frame\n",
    "            frame_path = frame_output_dir / f\"{frame_idx:05d}.jpg\"\n",
    "            cv2.imwrite(str(frame_path), frame)\n",
    "\n",
    "            # Run YOLOv8 detection (no classes filter here!)\n",
    "            results = model.predict(frame, verbose=False, device=device, imgsz=640)\n",
    "\n",
    "            # Manually filter for persons (class 0)\n",
    "            bboxes_xywh = []\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None and boxes.xyxy is not None:\n",
    "                    xyxy = boxes.xyxy.cpu().numpy()\n",
    "                    cls = boxes.cls.cpu().numpy()\n",
    "                    conf = boxes.conf.cpu().numpy()  # ‚Üê Add this line to get the confidences\n",
    "                    for (x1, y1, x2, y2), label, confidence in zip(xyxy, cls, conf):\n",
    "                        if int(label) == 0 and confidence >= CONFIDENCE_THRESHOLD:  # class 0 = person + confidence check\n",
    "                            x = float(x1)\n",
    "                            y = float(y1)\n",
    "                            w = float(x2 - x1)\n",
    "                            h = float(y2 - y1)\n",
    "                            bboxes_xywh.append([x, y, w, h])\n",
    "\n",
    "            bboxes_xywh = np.array(bboxes_xywh[:20], dtype=np.float32)\n",
    "\n",
    "            if bboxes_xywh.size == 0:\n",
    "                bboxes_xywh = np.empty((0, 4), dtype=np.float32)\n",
    "\n",
    "            bbox_path = bbox_output_dir / f\"{frame_idx:05d}.npy\"\n",
    "            np.save(str(bbox_path), bboxes_xywh)\n",
    "\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"‚úÖ Finished: {video_name} with {frame_idx} frames processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6c342-1e6e-4592-a1b6-1474ee511efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: RTM pose (body landmarks) with DLC:\n",
    "\n",
    "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
    "from deeplabcut.utils.video_processor import VideoProcessorCV\n",
    "from deeplabcut.utils.make_labeled_video import CreateVideo\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "import deeplabcut.utils\n",
    "deeplabcut.utils.tqdm = tqdm\n",
    "import shutil\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    "\n",
    "\n",
    "\n",
    "# === Model Configuration Paths ===\n",
    "path_model_config = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7_pytorch_config.yaml\")\n",
    "path_snapshot = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7.pt\")\n",
    "input_folder = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO/DLC\") # Change the folder here!!!\n",
    "\n",
    "# === Pose Model Settings ===\n",
    "device = \"mps\"  # Use Apple Silicon MPS\n",
    "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
    "runner = dlc_torch.get_pose_inference_runner(\n",
    "    pose_cfg,\n",
    "    snapshot_path=path_snapshot,\n",
    "    batch_size=4,\n",
    "    max_individuals=20,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# === Load video directories ===\n",
    "video_dirs = [d for d in input_folder.iterdir() if d.is_dir()]\n",
    "print(f\"üìÇ Found {len(video_dirs)} videos to process.\")\n",
    "\n",
    "# === Pose Estimation Loop ===\n",
    "for video_dir in tqdm(video_dirs, desc=\"Pose Estimation\", position=0):\n",
    "    print(f\"\\nüßç‚Äç‚ôÇÔ∏è Processing: {video_dir.name}\")\n",
    "    frame_dir = video_dir / \"frames\"\n",
    "    bbox_dir = video_dir / \"bboxes\"\n",
    "\n",
    "    frame_files = sorted(frame_dir.glob(\"*.jpg\"))\n",
    "    bbox_files = sorted(bbox_dir.glob(\"*.npy\"))\n",
    "\n",
    "    assert len(frame_files) == len(bbox_files), \"Mismatch between frames and bbox files.\"\n",
    "\n",
    "    output_csv_path = input_folder / f\"{video_dir.name}_predictions.csv\"\n",
    "    partial_predictions = {}\n",
    "\n",
    "    with tqdm(total=len(frame_files), desc=\"Pose estimation frames\", leave=False, position=1) as pbar:\n",
    "        for idx, (frame_file, bbox_file) in enumerate(zip(frame_files, bbox_files)):\n",
    "            frame = cv2.imread(str(frame_file))\n",
    "            if frame is None:\n",
    "                print(f\"‚ö†Ô∏è Failed to load frame: {frame_file}\")\n",
    "                continue\n",
    "\n",
    "            bboxes = np.load(str(bbox_file), allow_pickle=True)\n",
    "            frame_context = {\"bboxes\": bboxes}\n",
    "\n",
    "            # Run inference on single frame\n",
    "            pred = runner.inference([(frame, frame_context)])[0]\n",
    "            partial_predictions[idx] = pred\n",
    "\n",
    "            # Save every 100 frames\n",
    "            if (idx + 1) % 100 == 0 or (idx + 1) == len(frame_files):\n",
    "                df_partial = dlc_torch.build_predictions_dataframe(\n",
    "                    scorer=\"rtmpose-body7\",\n",
    "                    predictions=partial_predictions,\n",
    "                    parameters=dlc_torch.PoseDatasetParameters(\n",
    "                        bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                        unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "                        individuals=[f\"idv_{i}\" for i in range(20)]\n",
    "                    )\n",
    "                )\n",
    "                df_partial.to_csv(output_csv_path)\n",
    "                print(f\"üíæ Saved intermediate predictions at frame {idx+1}\")\n",
    "        \n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"‚úÖ Finished pose estimation: {video_dir.name}\")\n",
    "\n",
    "    create_labeled_video = False  # Set this to False if you DON'T want labeled videos!!!\n",
    "\n",
    "    # === Optional: Create labeled video IF NEEDED\n",
    "    if create_labeled_video:\n",
    "        original_video_path = Path(\"XXX\") / f\"{video_dir.name}.mp4\"\n",
    "        output_video_path = input_folder / f\"{video_dir.name}_labeled.mp4\"\n",
    "    \n",
    "        if original_video_path.exists():\n",
    "            clip = VideoProcessorCV(str(original_video_path), sname=str(output_video_path), codec=\"mp4v\")\n",
    "            df_final = dlc_torch.build_predictions_dataframe(\n",
    "                scorer=\"rtmpose-body7\",\n",
    "                predictions=partial_predictions,\n",
    "                parameters=dlc_torch.PoseDatasetParameters(\n",
    "                    bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                    unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "                    individuals=[f\"idv_{i}\" for i in range(20)]\n",
    "                )\n",
    "            )\n",
    "        \n",
    "            print(f\"üé¨ Creating labeled video: {output_video_path.name}\", end=\"\", flush=True)\n",
    "            \n",
    "            with suppress_stdout():\n",
    "                CreateVideo(\n",
    "                    clip,\n",
    "                    df_final,\n",
    "                    pcutoff=0.4,\n",
    "                    dotsize=5,\n",
    "                    colormap=\"rainbow\",\n",
    "                    bodyparts2plot=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                    trailpoints=0,\n",
    "                    cropping=False,\n",
    "                    x1=0,\n",
    "                    x2=clip.w,\n",
    "                    y1=0,\n",
    "                    y2=clip.h,\n",
    "                    bodyparts2connect=[\n",
    "                        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "                        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "                        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "                        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "                    ],\n",
    "                    skeleton_color=\"k\",\n",
    "                    draw_skeleton=True,\n",
    "                    displaycropped=False,\n",
    "                    color_by=\"bodypart\",\n",
    "                )\n",
    "            print(f\"üé¨ Labeled video saved: {output_video_path.name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Original video {original_video_path.name} not found, skipping labeled video.\")\n",
    "    \n",
    "    # Continue cleanup regardless of the flag\n",
    "    del partial_predictions\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nüéâ All pose estimations complete!\")\n",
    "\n",
    "# === REMOVE ALL DLC SUBFOLDERS (after all pose estimations are complete) ===\n",
    "for subfolder in input_folder.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(subfolder)\n",
    "            print(f\"üóëÔ∏è Deleted DLC subfolder: {subfolder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error deleting {subfolder}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af43e72-4e54-4f54-bbb5-a36de096ded3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplabcut)",
   "language": "python",
   "name": "deeplabcut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
