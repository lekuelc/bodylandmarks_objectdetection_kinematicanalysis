{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1be82-fb73-48ed-a226-ebadc49a255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do two steps: \n",
    "# 1: Export bounding boxes with YOLO: \n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.15\n",
    "\n",
    "# === Paths ===\n",
    "video_folder = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO\")\n",
    "output_base = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO/DLC\")\n",
    "\n",
    "if output_base.exists():\n",
    "    shutil.rmtree(output_base)\n",
    "output_base.mkdir(parents=True)\n",
    "\n",
    "video_files = list(video_folder.glob(\"*.mp4\")) + list(video_folder.glob(\"*.MP4\")) + \\\n",
    "              list(video_folder.glob(\"*.Mp4\")) + list(video_folder.glob(\"*.mP4\"))\n",
    "\n",
    "print(f\"üé• Found {len(video_files)} video(s) to process.\")\n",
    "if not video_files:\n",
    "    raise FileNotFoundError(\"No video files found!\")\n",
    "\n",
    "# === Load YOLOv11 model\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Using device: {device}\")\n",
    "model = YOLO(\"yolo11l.pt\")\n",
    "\n",
    "# === Detection Loop ===\n",
    "for i, video_path in enumerate(tqdm(video_files, desc=\"YOLO Detection\", position=0)):\n",
    "    print(f\"\\nüìπ [{i+1}/{len(video_files)}] Processing: {video_path.name}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ö†Ô∏è Could not open video: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_name = video_path.stem\n",
    "    frame_output_dir = output_base / video_name / \"frames\"\n",
    "    bbox_output_dir = output_base / video_name / \"bboxes\"\n",
    "    frame_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    bbox_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"Processing frames\", leave=False, position=1) as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                break\n",
    "\n",
    "            # Save frame\n",
    "            frame_path = frame_output_dir / f\"{frame_idx:05d}.jpg\"\n",
    "            cv2.imwrite(str(frame_path), frame)\n",
    "\n",
    "            # Run YOLOv8 detection (no classes filter here!)\n",
    "            results = model.predict(frame, verbose=False, device=device, imgsz=640)\n",
    "\n",
    "            # Manually filter for persons (class 0)\n",
    "            bboxes_xywh = []\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None and boxes.xyxy is not None:\n",
    "                    xyxy = boxes.xyxy.cpu().numpy()\n",
    "                    cls = boxes.cls.cpu().numpy()\n",
    "                    conf = boxes.conf.cpu().numpy()  # ‚Üê Add this line to get the confidences\n",
    "                    for (x1, y1, x2, y2), label, confidence in zip(xyxy, cls, conf):\n",
    "                        if int(label) == 0 and confidence >= CONFIDENCE_THRESHOLD:  # class 0 = person + confidence check\n",
    "                            x = float(x1)\n",
    "                            y = float(y1)\n",
    "                            w = float(x2 - x1)\n",
    "                            h = float(y2 - y1)\n",
    "                            bboxes_xywh.append([x, y, w, h])\n",
    "\n",
    "            bboxes_xywh = np.array(bboxes_xywh[:20], dtype=np.float32)\n",
    "\n",
    "            if bboxes_xywh.size == 0:\n",
    "                bboxes_xywh = np.empty((0, 4), dtype=np.float32)\n",
    "\n",
    "            bbox_path = bbox_output_dir / f\"{frame_idx:05d}.npy\"\n",
    "            np.save(str(bbox_path), bboxes_xywh)\n",
    "\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"‚úÖ Finished: {video_name} with {frame_idx} frames processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6c342-1e6e-4592-a1b6-1474ee511efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: RTM pose with DLC:\n",
    "\n",
    "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
    "from deeplabcut.utils.video_processor import VideoProcessorCV\n",
    "from deeplabcut.utils.make_labeled_video import CreateVideo\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "import deeplabcut.utils\n",
    "deeplabcut.utils.tqdm = tqdm\n",
    "import shutil\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    "\n",
    "\n",
    "\n",
    "# === Model Configuration Paths ===\n",
    "path_model_config = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7_pytorch_config.yaml\")\n",
    "path_snapshot = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7.pt\")\n",
    "input_folder = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO/DLC\") # Change the folder here!!!\n",
    "\n",
    "# === Pose Model Settings ===\n",
    "device = \"mps\"  # Use Apple Silicon MPS\n",
    "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
    "runner = dlc_torch.get_pose_inference_runner(\n",
    "    pose_cfg,\n",
    "    snapshot_path=path_snapshot,\n",
    "    batch_size=4,\n",
    "    max_individuals=20,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# === Load video directories ===\n",
    "video_dirs = [d for d in input_folder.iterdir() if d.is_dir()]\n",
    "print(f\"üìÇ Found {len(video_dirs)} videos to process.\")\n",
    "\n",
    "# === Pose Estimation Loop ===\n",
    "for video_dir in tqdm(video_dirs, desc=\"Pose Estimation\", position=0):\n",
    "    print(f\"\\nüßç‚Äç‚ôÇÔ∏è Processing: {video_dir.name}\")\n",
    "    frame_dir = video_dir / \"frames\"\n",
    "    bbox_dir = video_dir / \"bboxes\"\n",
    "\n",
    "    frame_files = sorted(frame_dir.glob(\"*.jpg\"))\n",
    "    bbox_files = sorted(bbox_dir.glob(\"*.npy\"))\n",
    "\n",
    "    assert len(frame_files) == len(bbox_files), \"Mismatch between frames and bbox files.\"\n",
    "\n",
    "    output_csv_path = input_folder / f\"{video_dir.name}_predictions.csv\"\n",
    "    partial_predictions = {}\n",
    "\n",
    "    with tqdm(total=len(frame_files), desc=\"Pose estimation frames\", leave=False, position=1) as pbar:\n",
    "        for idx, (frame_file, bbox_file) in enumerate(zip(frame_files, bbox_files)):\n",
    "            frame = cv2.imread(str(frame_file))\n",
    "            if frame is None:\n",
    "                print(f\"‚ö†Ô∏è Failed to load frame: {frame_file}\")\n",
    "                continue\n",
    "\n",
    "            bboxes = np.load(str(bbox_file), allow_pickle=True)\n",
    "            frame_context = {\"bboxes\": bboxes}\n",
    "\n",
    "            # Run inference on single frame\n",
    "            pred = runner.inference([(frame, frame_context)])[0]\n",
    "            partial_predictions[idx] = pred\n",
    "\n",
    "            # Save every 100 frames\n",
    "            if (idx + 1) % 100 == 0 or (idx + 1) == len(frame_files):\n",
    "                df_partial = dlc_torch.build_predictions_dataframe(\n",
    "                    scorer=\"rtmpose-body7\",\n",
    "                    predictions=partial_predictions,\n",
    "                    parameters=dlc_torch.PoseDatasetParameters(\n",
    "                        bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                        unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "                        individuals=[f\"idv_{i}\" for i in range(20)]\n",
    "                    )\n",
    "                )\n",
    "                df_partial.to_csv(output_csv_path)\n",
    "                print(f\"üíæ Saved intermediate predictions at frame {idx+1}\")\n",
    "        \n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"‚úÖ Finished pose estimation: {video_dir.name}\")\n",
    "\n",
    "    create_labeled_video = False  # Set this to False if you DON'T want labeled videos!!!\n",
    "\n",
    "    # === Optional: Create labeled video IF NEEDED\n",
    "    if create_labeled_video:\n",
    "        original_video_path = Path(\"XXX\") / f\"{video_dir.name}.mp4\"\n",
    "        output_video_path = input_folder / f\"{video_dir.name}_labeled.mp4\"\n",
    "    \n",
    "        if original_video_path.exists():\n",
    "            clip = VideoProcessorCV(str(original_video_path), sname=str(output_video_path), codec=\"mp4v\")\n",
    "            df_final = dlc_torch.build_predictions_dataframe(\n",
    "                scorer=\"rtmpose-body7\",\n",
    "                predictions=partial_predictions,\n",
    "                parameters=dlc_torch.PoseDatasetParameters(\n",
    "                    bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                    unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "                    individuals=[f\"idv_{i}\" for i in range(20)]\n",
    "                )\n",
    "            )\n",
    "        \n",
    "            print(f\"üé¨ Creating labeled video: {output_video_path.name}\", end=\"\", flush=True)\n",
    "            \n",
    "            with suppress_stdout():\n",
    "                CreateVideo(\n",
    "                    clip,\n",
    "                    df_final,\n",
    "                    pcutoff=0.4,\n",
    "                    dotsize=5,\n",
    "                    colormap=\"rainbow\",\n",
    "                    bodyparts2plot=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                    trailpoints=0,\n",
    "                    cropping=False,\n",
    "                    x1=0,\n",
    "                    x2=clip.w,\n",
    "                    y1=0,\n",
    "                    y2=clip.h,\n",
    "                    bodyparts2connect=[\n",
    "                        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "                        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "                        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "                        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "                    ],\n",
    "                    skeleton_color=\"k\",\n",
    "                    draw_skeleton=True,\n",
    "                    displaycropped=False,\n",
    "                    color_by=\"bodypart\",\n",
    "                )\n",
    "            print(f\"üé¨ Labeled video saved: {output_video_path.name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Original video {original_video_path.name} not found, skipping labeled video.\")\n",
    "    \n",
    "    # Continue cleanup regardless of the flag\n",
    "    del partial_predictions\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nüéâ All pose estimations complete!\")\n",
    "\n",
    "# === REMOVE ALL DLC SUBFOLDERS (after all pose estimations are complete) ===\n",
    "for subfolder in input_folder.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(subfolder)\n",
    "            print(f\"üóëÔ∏è Deleted DLC subfolder: {subfolder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error deleting {subfolder}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e180ed-e765-4858-90b9-cbeebbfaece8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: REASSIGNMENT === UPDATED SCRIPT: Add Full Skeleton ===\n",
    "# === FINAL CLEAN SCRIPT: Stable ID Assignment + Pose Matching + True Reassignment (Skeleton to Skeleton) ===\n",
    "\n",
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === SETTINGS ===\n",
    "MATCH_THRESHOLD_FRAME = 150          # Pixel-based threshold if not normalizing\n",
    "USE_NORMALIZED_DISTANCE = True       # Switch for normalized matching, otherwise fixed distance matching\n",
    "NORMALIZED_MATCH_THRESHOLD = 0.3     # Threshold if normalizing distances\n",
    "\n",
    "# === LOAD VIDEO FILES ===\n",
    "base_folder = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO\")\n",
    "video_files = list(base_folder.glob(\"*.mp4\")) + list(base_folder.glob(\"*.MP4\"))\n",
    "\n",
    "# === MAIN LOOP: PROCESS EACH VIDEO ===\n",
    "for video_path in video_files:\n",
    "    print(f\"\\nüé• Processing video: {video_path.name}\")\n",
    "\n",
    "    INPUT_PATH = video_path\n",
    "\n",
    "    # === LOAD TRACKING DATA (Bounding Boxes) ===\n",
    "    tracking_df = pd.read_csv(INPUT_PATH.parent / \"YOLO\" / (INPUT_PATH.stem + \".csv\"))\n",
    "\n",
    "    # === LOAD POSE ESTIMATION DATA (Skeletons) ===\n",
    "    pose_df = pd.read_csv(INPUT_PATH.parent / \"DLC\" / (INPUT_PATH.stem + \"_predictions.csv\"), low_memory=False)\n",
    "\n",
    "    # === PREPARE POSE DATAFRAME ===\n",
    "    multi_index = pd.MultiIndex.from_arrays(pose_df.iloc[0:3].values, names=[\"individual\", \"bodypart\", \"coord\"])\n",
    "    pose_data_cleaned = pose_df.iloc[3:].copy()\n",
    "    pose_data_cleaned.columns = multi_index\n",
    "    pose_data_cleaned.reset_index(drop=True, inplace=True)\n",
    "    pose_data_cleaned[\"Frame\"] = pose_data_cleaned.index.astype(int)\n",
    "\n",
    "    # === DETECT BODY PARTS (skip non-body columns) ===\n",
    "    bodyparts = pose_data_cleaned.columns.get_level_values(\"bodypart\").unique()\n",
    "    bodyparts = [bp for bp in bodyparts if bp not in [\"Frame\", \"scorer\"]]\n",
    "\n",
    "    # === FINAL DATA COLLECTOR ===\n",
    "    final_data = []\n",
    "\n",
    "    # === FRAME-BY-FRAME PROCESSING ===\n",
    "    for frame_idx, pose_row in tqdm(pose_data_cleaned.iterrows(), total=len(pose_data_cleaned), desc=f\"Processing {INPUT_PATH.stem}\"):\n",
    "        frame_num = int(pose_row[\"Frame\"].iloc[0] if isinstance(pose_row[\"Frame\"], pd.Series) else pose_row[\"Frame\"])\n",
    "        frame_boxes = tracking_df[tracking_df.Frame == frame_num]\n",
    "\n",
    "        # === EXTRACT ALL SKELETON CENTERS IN THIS FRAME ===\n",
    "        skeleton_centers = {}\n",
    "        individuals = pose_row.index.get_level_values(\"individual\").unique()\n",
    "        individuals = [ind for ind in individuals if ind.startswith(\"idv_\")]\n",
    "\n",
    "        for ind in individuals:\n",
    "            keypoints = []\n",
    "            for bp in bodyparts:\n",
    "                try:\n",
    "                    x = float(pose_row[(ind, bp, \"x\")])\n",
    "                    y = float(pose_row[(ind, bp, \"y\")])\n",
    "                    if pd.notna(x) and pd.notna(y):\n",
    "                        keypoints.append((x, y))\n",
    "                except:\n",
    "                    continue\n",
    "            if keypoints:\n",
    "                cx, cy = np.mean(keypoints, axis=0)  # Compute mean x, mean y = skeleton center\n",
    "                skeleton_centers[ind] = (cx, cy)\n",
    "\n",
    "        # === MATCH EACH BOUNDING BOX TO NEAREST SKELETON ===\n",
    "        for _, box in frame_boxes.iterrows():\n",
    "            x_center = (box.X1 + box.X2) / 2\n",
    "            y_center = (box.Y1 + box.Y2) / 2\n",
    "            pid = int(box.Person_ID)\n",
    "\n",
    "            best_match = None\n",
    "            min_distance = float('inf')\n",
    "\n",
    "            for skel_id, (cx, cy) in skeleton_centers.items():\n",
    "                center_distance = np.linalg.norm(np.array([cx, cy]) - np.array([x_center, y_center]))\n",
    "\n",
    "                # === OPTIONAL: Normalize distance by bounding box size ===\n",
    "                if USE_NORMALIZED_DISTANCE:\n",
    "                    bbox_width = box.X2 - box.X1\n",
    "                    bbox_height = box.Y2 - box.Y1\n",
    "                    avg_bbox_size = (bbox_width + bbox_height) / 2\n",
    "                    if avg_bbox_size > 0:\n",
    "                        center_distance /= avg_bbox_size\n",
    "\n",
    "                threshold = NORMALIZED_MATCH_THRESHOLD if USE_NORMALIZED_DISTANCE else MATCH_THRESHOLD_FRAME\n",
    "\n",
    "                # === KEEP CLOSEST SKELETON UNDER THRESHOLD ===\n",
    "                if center_distance < min_distance and center_distance < threshold:\n",
    "                    best_match = skel_id\n",
    "                    min_distance = center_distance\n",
    "\n",
    "            # === SAVE MATCH INFORMATION ===\n",
    "            data = {\n",
    "                \"Frame\": frame_num,\n",
    "                \"Old_ID\": pid,\n",
    "                \"New_ID\": pid,\n",
    "                \"X1\": box.X1,\n",
    "                \"Y1\": box.Y1,\n",
    "                \"X2\": box.X2,\n",
    "                \"Y2\": box.Y2,\n",
    "            }\n",
    "\n",
    "            if best_match:\n",
    "                skel_num = best_match.split(\"_\")[1]\n",
    "                for bp in bodyparts:\n",
    "                    x = pose_row.get((f\"idv_{skel_num}\", bp, \"x\"), np.nan)\n",
    "                    y = pose_row.get((f\"idv_{skel_num}\", bp, \"y\"), np.nan)\n",
    "                    conf = pose_row.get((f\"idv_{skel_num}\", bp, \"likelihood\"), np.nan)\n",
    "                    data[f\"{bp}_x\"] = x\n",
    "                    data[f\"{bp}_y\"] = y\n",
    "                    data[f\"{bp}_conf\"] = conf\n",
    "\n",
    "            final_data.append(data)\n",
    "\n",
    "    # === SAVE FINAL CORRECTED CSV ===\n",
    "    final_df = pd.DataFrame(final_data)\n",
    "    final_df = final_df.sort_values([\"Frame\", \"New_ID\"])\n",
    "    final_df.drop(columns=[\"bodyparts_x\", \"bodyparts_y\", \"bodyparts_conf\", \"_x\", \"_y\", \"_conf\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    OUTPUT_FOLDER = INPUT_PATH.parent / \"corrected\"\n",
    "    OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_csv_name = f\"{INPUT_PATH.stem}_corrected.csv\"\n",
    "    final_df.to_csv(OUTPUT_FOLDER / output_csv_name, index=False)\n",
    "\n",
    "    print(\"\\u2705 Final corrected CSV saved for:\", video_path.name)\n",
    "\n",
    "print(\"\\nüéâ All sessions processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b11956-ee9e-473d-83f5-e8855e1d85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge files: person classification and object classfication: \n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === PATHS ===\n",
    "main_folder = Path(\"/Users/Christian/Downloads/Javelin training/analysis/corrected\")\n",
    "object_folder = Path(\"/Users/Christian/Downloads/Javelin training/analysis/YOLO_Object\")\n",
    "merged_folder = main_folder / \"with_objects\"\n",
    "merged_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === PROCESSING LOOP ===\n",
    "for corrected_file in main_folder.glob(\"*_corrected.csv\"):\n",
    "    base_name = corrected_file.stem.replace(\"_corrected\", \"\")\n",
    "    object_file = object_folder / f\"{base_name}.csv\"\n",
    "\n",
    "    if not object_file.exists():\n",
    "        print(f\"Skipping {base_name} ‚Äî no object file found.\")\n",
    "        continue\n",
    "\n",
    "    # Load both dataframes\n",
    "    df_people = pd.read_csv(corrected_file)\n",
    "    df_object = pd.read_csv(object_file)\n",
    "\n",
    "    # Compute center points\n",
    "    df_object[\"center_x\"] = (df_object[\"X1\"] + df_object[\"X2\"]) / 2\n",
    "    df_object[\"center_y\"] = (df_object[\"Y1\"] + df_object[\"Y2\"]) / 2\n",
    "\n",
    "    # Pivot: one row per frame, columns like Tip_center_x, Handle_center_y, etc.\n",
    "    # Keep only needed columns\n",
    "    df_object_slim = df_object[[\"Frame\", \"Label\", \"X1\", \"Y1\", \"X2\", \"Y2\", \"center_x\", \"center_y\"]]\n",
    "    \n",
    "    # Pivot: one row per frame, columns like Tip_X1, Tip_center_x, etc.\n",
    "    df_object_pivot = df_object_slim.pivot_table(\n",
    "        index=\"Frame\",\n",
    "        columns=\"Label\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Flatten multi-index columns\n",
    "    df_object_pivot.columns = [f\"{label}_{coord}\" for coord, label in df_object_pivot.columns]\n",
    "    df_object_pivot.reset_index(inplace=True)\n",
    "\n",
    "    # Merge person + object center info\n",
    "    df_merged = df_people.merge(df_object_pivot, on=\"Frame\", how=\"left\")\n",
    "\n",
    "    # Save to new folder\n",
    "    out_path = merged_folder / f\"{base_name}_merged.csv\"\n",
    "    df_merged.to_csv(out_path, index=False)\n",
    "    print(f\"‚úÖ Merged and saved: {out_path.name}\")\n",
    "\n",
    "print(\"\\nüéâ All sessions processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415ca54-77e3-423c-ac63-893f6388acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for visualization of how many were reassigned, not really necessary\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your final corrected file\n",
    "csv_path = \"/Users/Christian/Downloads/Fabrice Videos/1b/analysis/corrected/processed_1b mit Intervention_corrected.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Calculate basic statistics\n",
    "total_frames = df['Frame'].nunique()\n",
    "total_ids = len(df)\n",
    "reassigned = df['Tried_Reassignment'].sum()\n",
    "successful_reassigned = (df['Old_ID'] != df['New_ID']).sum()\n",
    "\n",
    "# Print results\n",
    "print(f\"Total frames: {total_frames}\")\n",
    "print(f\"Total ID entries: {total_ids}\")\n",
    "print(f\"Reassignment attempts: {reassigned}\")\n",
    "print(f\"Successful reassignments: {successful_reassigned}\")\n",
    "print(f\"Reassignment attempt rate: {100 * reassigned / total_ids:.2f}% of all IDs\")\n",
    "print(f\"Successful reassignment rate: {100 * successful_reassigned / total_ids:.2f}% of all IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d28cd-dd2b-49e0-af83-c913a7a8486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UPDATED SCRIPT: FINAL CONFIRMATION VIDEO (for reassigned IDs and likelihoods) ===\n",
    "\n",
    "# === FINAL CONFIRMATION VIDEOS (MULTIPLE FILES, ONLY FINAL_ID VISIBLE) ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === SETTINGS ===\n",
    "VIDEO_INPUT_FOLDER = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO\")\n",
    "FINAL_DF_INPUT_FOLDER = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO/corrected\")\n",
    "OUTPUT_FOLDER = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO/corrected\")\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "ONLY_SHOW_HIGH_IDS = False  # Set to False to show all IDs (not only IDs > 1999)\n",
    "\n",
    "# Target bodyparts list\n",
    "target_bodyparts = [\n",
    "    \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n",
    "    \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n",
    "    \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n",
    "    \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n",
    "]\n",
    "\n",
    "# === PROCESS EACH FINAL CSV FILE ===\n",
    "for final_csv in FINAL_DF_INPUT_FOLDER.glob(\"processed_*_corrected.csv\"):\n",
    "    session_name = final_csv.stem.replace(\"_corrected\", \"\")\n",
    "    print(f\"\\nüîµ Processing session: {session_name}\")\n",
    "\n",
    "    # Define corresponding video\n",
    "    video_filename = f\"{session_name}.MP4\"\n",
    "    video_path = VIDEO_INPUT_FOLDER / video_filename\n",
    "\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Video {video_filename} not found! Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load final corrected CSV\n",
    "    final_df = pd.read_csv(final_csv)\n",
    "\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    frame_limit = int(fps * 180)  # Only process first 30 seconds\n",
    "\n",
    "    # Define output video path\n",
    "    out_video_path = OUTPUT_FOLDER / f\"visual_inspection_{session_name}.mp4\"\n",
    "\n",
    "    out_video = cv2.VideoWriter(\n",
    "        str(out_video_path),\n",
    "        cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "        fps,\n",
    "        (frame_width, frame_height)\n",
    "    )\n",
    "\n",
    "    colors = {}  # Final_ID ‚Üí color mapping\n",
    "    print(f\"üé• Creating video for {session_name}...\")\n",
    "\n",
    "    progress_bar = tqdm(total=min(total_frames, frame_limit), desc=f\"Creating {session_name}\", unit=\"frame\", leave=True)\n",
    "\n",
    "    right_wrist_start_y = {}\n",
    "    \n",
    "    right_wrist_start_y = {}\n",
    "\n",
    "    for frame_idx in range(min(total_frames, frame_limit)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        frame_data = final_df[final_df.Frame == frame_idx]\n",
    "    \n",
    "        # Draw pink dot at object center (if available)\n",
    "        if \"Object_center_x\" in frame_data.columns and not frame_data[\"Object_center_x\"].isna().all():\n",
    "            obj_centers = frame_data[[\"Object_center_x\", \"Object_center_y\"]].dropna().drop_duplicates()\n",
    "            for _, obj_row in obj_centers.iterrows():\n",
    "                cx, cy = int(obj_row[\"Object_center_x\"]), int(obj_row[\"Object_center_y\"])\n",
    "                cv2.circle(frame, (cx, cy), 24, (255, 105, 180), -1)  # pink dot\n",
    "    \n",
    "        for _, row in frame_data.iterrows():\n",
    "            final_id = int(row[\"New_ID\"])\n",
    "    \n",
    "            # Skip drawing if ONLY_SHOW_HIGH_IDS is True and ID <= 1999\n",
    "            if ONLY_SHOW_HIGH_IDS and final_id <= 1999:\n",
    "                continue\n",
    "    \n",
    "            color = colors.setdefault(final_id, tuple(np.random.randint(0, 255, size=3).tolist()))\n",
    "    \n",
    "            # Draw bounding box\n",
    "            x1, y1, x2, y2 = int(row[\"X1\"]), int(row[\"Y1\"]), int(row[\"X2\"]), int(row[\"Y2\"])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "    \n",
    "            # Draw ID label (ONLY Final_ID)\n",
    "            x_center = (x1 + x2) // 2\n",
    "            y_center = (y1 + y2) // 2\n",
    "            id_label = f\"ID {final_id}\"\n",
    "            cv2.putText(frame, id_label, (x_center + 10, y_center - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 3)\n",
    "    \n",
    "            # ----- NEW: Calculate right leg length -----\n",
    "            leg_length = np.nan\n",
    "            if (not np.isnan(row[\"right_ankle_x\"]) and not np.isnan(row[\"right_ankle_y\"]) and\n",
    "                not np.isnan(row[\"right_hip_x\"]) and not np.isnan(row[\"right_hip_y\"])):\n",
    "                leg_length = np.sqrt(\n",
    "                    (row[\"right_ankle_x\"] - row[\"right_hip_x\"])**2 +\n",
    "                    (row[\"right_ankle_y\"] - row[\"right_hip_y\"])**2\n",
    "                )\n",
    "            if not np.isnan(leg_length):\n",
    "                label_leg = f\"Leg: {leg_length:.1f}\"\n",
    "                cv2.putText(frame, label_leg, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    \n",
    "            # ----- NEW: Calculate normalized change in right wrist y -----\n",
    "            rw_y = row.get(\"right_wrist_y\", np.nan)\n",
    "            # Store the start y for each ID\n",
    "            if final_id not in right_wrist_start_y and not np.isnan(rw_y):\n",
    "                right_wrist_start_y[final_id] = rw_y\n",
    "            delta_rw = np.nan\n",
    "            if (final_id in right_wrist_start_y and not np.isnan(rw_y)\n",
    "                and not np.isnan(leg_length) and leg_length > 1):\n",
    "                delta_rw = (rw_y - right_wrist_start_y[final_id]) / leg_length\n",
    "            if not np.isnan(delta_rw):\n",
    "                label2 = f\"Œîwrist/leg: {delta_rw:+.2f}\"\n",
    "                cv2.putText(frame, label2, (x1, y1 - 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 200, 0), 2)\n",
    "    \n",
    "            # Draw body markers\n",
    "            keypoints = {}\n",
    "            for part in target_bodyparts:\n",
    "                x = row.get(f\"{part}_x\", np.nan)\n",
    "                y = row.get(f\"{part}_y\", np.nan)\n",
    "                conf = row.get(f\"{part}_conf\", np.nan)\n",
    "    \n",
    "                if not np.isnan(x) and not np.isnan(y) and (conf > 0.05):\n",
    "                    keypoints[part] = (int(x), int(y))\n",
    "                    cv2.circle(frame, (int(x), int(y)), 5, color, -1)\n",
    "    \n",
    "                    # Display the right wrist y-coordinate\n",
    "                    if part == \"right_wrist\":\n",
    "                        label = f\"Y={int(y)}\"\n",
    "                        cv2.putText(\n",
    "                            frame,\n",
    "                            label,\n",
    "                            (int(x) + 10, int(y) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (0, 255, 255),  # Yellow for visibility\n",
    "                            2\n",
    "                        )\n",
    "    \n",
    "            # Draw skeleton connections\n",
    "            skeleton_connections = [\n",
    "                (\"left_shoulder\", \"right_shoulder\"), (\"left_shoulder\", \"left_elbow\"), (\"left_elbow\", \"left_wrist\"),\n",
    "                (\"right_shoulder\", \"right_elbow\"), (\"right_elbow\", \"right_wrist\"),\n",
    "                (\"left_shoulder\", \"left_hip\"), (\"right_shoulder\", \"right_hip\"),\n",
    "                (\"left_hip\", \"right_hip\"), (\"left_hip\", \"left_knee\"), (\"left_knee\", \"left_ankle\"),\n",
    "                (\"right_hip\", \"right_knee\"), (\"right_knee\", \"right_ankle\"),\n",
    "                (\"nose\", \"left_eye\"), (\"nose\", \"right_eye\"),\n",
    "                (\"left_eye\", \"left_ear\"), (\"right_eye\", \"right_ear\")\n",
    "            ]\n",
    "    \n",
    "            for bp1, bp2 in skeleton_connections:\n",
    "                if bp1 in keypoints and bp2 in keypoints:\n",
    "                    cv2.line(frame, keypoints[bp1], keypoints[bp2], color, 2)\n",
    "    \n",
    "        out_video.write(frame)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    out_video.release()\n",
    "    progress_bar.close()\n",
    "\n",
    "    print(f\"‚úÖ Final video saved: {out_video_path.name}\")\n",
    "\n",
    "print(\"\\nüéâ All sessions processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1b5dd-5337-4852-8d0c-17e8f1853212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP HERE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860d6bd-7e56-4755-9fdc-b22a04d28961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR JAVELIN THROWING # === UPDATED SCRIPT: FINAL CONFIRMATION VIDEO (for reassigned IDs and likelihoods) ===\n",
    "\n",
    "# === FINAL CONFIRMATION VIDEOS (MULTIPLE FILES, ONLY FINAL_ID VISIBLE) ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === SETTINGS ===\n",
    "VIDEO_INPUT_FOLDER = Path(\"/Users/Christian/Downloads/Javelin training/analysis\")\n",
    "FINAL_DF_INPUT_FOLDER = Path(\"/Users/Christian/Downloads/Javelin training/analysis/corrected/with_objects\")\n",
    "OUTPUT_FOLDER = Path(\"/Users/Christian/Downloads/Javelin training/analysis/corrected/with_objects\")\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ONLY_SHOW_HIGH_IDS = False  # Set to False to show all IDs (not only IDs > 1999)\n",
    "\n",
    "# Target bodyparts list\n",
    "target_bodyparts = [\n",
    "    \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n",
    "    \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n",
    "    \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n",
    "    \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"\n",
    "]\n",
    "\n",
    "# === PROCESS EACH FINAL CSV FILE ===\n",
    "for final_csv in FINAL_DF_INPUT_FOLDER.glob(\"*_merged.csv\"):\n",
    "    session_name = final_csv.stem.replace(\"_merged\", \"\")\n",
    "    print(f\"\\nüîµ Processing session: {session_name}\")\n",
    "\n",
    "    # Define corresponding video\n",
    "    video_filename = f\"{session_name}.MP4\"\n",
    "    video_path = VIDEO_INPUT_FOLDER / video_filename\n",
    "\n",
    "    if not video_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Video {video_filename} not found! Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load final corrected CSV\n",
    "    final_df = pd.read_csv(final_csv)\n",
    "\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_limit = int(fps * 180)  # Only process first 30 seconds\n",
    "\n",
    "    # Define output video path\n",
    "    out_video_path = OUTPUT_FOLDER / f\"visual_inspection_{session_name}.mp4\"\n",
    "\n",
    "    out_video = cv2.VideoWriter(\n",
    "        str(out_video_path),\n",
    "        cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "        fps,\n",
    "        (frame_width, frame_height)\n",
    "    )\n",
    "\n",
    "    colors = {}  # Final_ID ‚Üí color mapping\n",
    "    print(f\"üé• Creating video for {session_name}...\")\n",
    "\n",
    "    progress_bar = tqdm(total=min(total_frames, frame_limit), desc=f\"Creating {session_name}\", unit=\"frame\", leave=True)\n",
    "\n",
    "    print(f\"üìä CSV frame range: {final_df['Frame'].min()} to {final_df['Frame'].max()}\")\n",
    "    print(f\"üéûÔ∏è Video has {total_frames} frames\")\n",
    "\n",
    "    for frame_idx in range(min(total_frames, frame_limit)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_data = final_df[final_df.Frame == frame_idx]\n",
    "\n",
    "\n",
    "        # Draw Tip, Handle, and Tail object centers with distinct colors\n",
    "        object_labels = [\"Tip\", \"Handle\", \"Tail\"]\n",
    "        object_colors = {\n",
    "            \"Tip\": (255, 105, 180),     # Pink\n",
    "            \"Handle\": (0, 255, 0),      # Green\n",
    "            \"Tail\": (0, 165, 255)       # Orange\n",
    "        }\n",
    "        \n",
    "        for label in object_labels:\n",
    "            x_col = f\"{label}_center_x\"\n",
    "            y_col = f\"{label}_center_y\"\n",
    "        \n",
    "            if x_col in frame_data.columns and y_col in frame_data.columns:\n",
    "                obj_coords = frame_data[[x_col, y_col]].dropna().drop_duplicates()\n",
    "        \n",
    "                for _, obj in obj_coords.iterrows():\n",
    "                    cx, cy = int(obj[x_col]), int(obj[y_col])\n",
    "                    cv2.circle(frame, (cx, cy), 20, object_colors[label], -1)\n",
    "                    cv2.putText(frame, label, (cx + 10, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, object_colors[label], 2)\n",
    "\n",
    "\n",
    "        for _, row in frame_data.iterrows():\n",
    "            final_id = int(row[\"New_ID\"])\n",
    "\n",
    "            # Skip drawing if ONLY_SHOW_HIGH_IDS is True and ID <= 1999\n",
    "            if ONLY_SHOW_HIGH_IDS and final_id <= 1999:\n",
    "                continue\n",
    "            \n",
    "            color = (0, 0, 255)\n",
    "\n",
    "            \n",
    "            # Draw bounding box\n",
    "            x1, y1, x2, y2 = int(row[\"X1\"]), int(row[\"Y1\"]), int(row[\"X2\"]), int(row[\"Y2\"])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "            # Draw ID label (ONLY Final_ID)\n",
    "            x_center = (x1 + x2) // 2\n",
    "            y_center = (y1 + y2) // 2\n",
    "            id_label = f\"ID {final_id}\"\n",
    "            cv2.putText(frame, id_label, (x_center + 10, y_center - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 3)\n",
    "\n",
    "            # Draw body markers\n",
    "            keypoints = {}\n",
    "            for part in target_bodyparts:\n",
    "                x = row.get(f\"{part}_x\", np.nan)\n",
    "                y = row.get(f\"{part}_y\", np.nan)\n",
    "                conf = row.get(f\"{part}_conf\", np.nan)\n",
    "\n",
    "                if not np.isnan(x) and not np.isnan(y) and (conf > 0.05):\n",
    "                    keypoints[part] = (int(x), int(y))\n",
    "                    cv2.circle(frame, (int(x), int(y)), 5, color, -1)\n",
    "\n",
    "                    if part == \"right_wrist\":\n",
    "                        label = f\"Y={int(y)}\"\n",
    "                        cv2.putText(\n",
    "                            frame,\n",
    "                            label,\n",
    "                            (int(x) + 10, int(y) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (0, 255, 255),  # Yellow for visibility\n",
    "                            2\n",
    "                        )\n",
    "\n",
    "            # Draw skeleton connections\n",
    "            skeleton_connections = [\n",
    "                (\"left_shoulder\", \"right_shoulder\"), (\"left_shoulder\", \"left_elbow\"), (\"left_elbow\", \"left_wrist\"),\n",
    "                (\"right_shoulder\", \"right_elbow\"), (\"right_elbow\", \"right_wrist\"),\n",
    "                (\"left_shoulder\", \"left_hip\"), (\"right_shoulder\", \"right_hip\"),\n",
    "                (\"left_hip\", \"right_hip\"), (\"left_hip\", \"left_knee\"), (\"left_knee\", \"left_ankle\"),\n",
    "                (\"right_hip\", \"right_knee\"), (\"right_knee\", \"right_ankle\"),\n",
    "                (\"nose\", \"left_eye\"), (\"nose\", \"right_eye\"),\n",
    "                (\"left_eye\", \"left_ear\"), (\"right_eye\", \"right_ear\")\n",
    "            ]\n",
    "\n",
    "\n",
    "            for bp1, bp2 in skeleton_connections:\n",
    "                if bp1 in keypoints and bp2 in keypoints:\n",
    "                    cv2.line(frame, keypoints[bp1], keypoints[bp2], color, 2)\n",
    "\n",
    "        out_video.write(frame)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    out_video.release()\n",
    "    progress_bar.close()\n",
    "\n",
    "    print(f\"‚úÖ Final video saved: {out_video_path.name}\")\n",
    "\n",
    "print(\"\\nüéâ All sessions processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed975ed9-10cd-4c13-81ad-27892c1c957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop here: below are old code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109606d-8817-4db7-808d-f7188ccc8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: REASSIGNMENT === UPDATED SCRIPT: Add Full Skeleton ===\n",
    "# === FINAL CLEAN SCRIPT: Stable ID Assignment + Pose Matching + Old/New ID Tracking + Likelihoods ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === SIMPLE KALMAN FILTER CLASS ===\n",
    "class SimpleKalman2D:\n",
    "    def __init__(self, x, y):\n",
    "        self.state = np.array([x, y, 0, 0], dtype=float)\n",
    "        self.P = np.eye(4) * 1000\n",
    "        self.F = np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]])\n",
    "        self.H = np.array([[1,0,0,0],[0,1,0,0]])\n",
    "        self.R = np.eye(2) * 10\n",
    "        self.Q = np.eye(4) * 0.01\n",
    "\n",
    "    def predict(self):\n",
    "        self.state = self.F @ self.state\n",
    "        self.P = self.F @ self.P @ self.F.T + self.Q\n",
    "        return self.state[0], self.state[1]\n",
    "\n",
    "    def update(self, x, y):\n",
    "        z = np.array([x, y])\n",
    "        y_ = z - (self.H @ self.state)\n",
    "        S = self.H @ self.P @ self.H.T + self.R\n",
    "        K = self.P @ self.H.T @ np.linalg.inv(S)\n",
    "        self.state += K @ y_\n",
    "        self.P = (np.eye(4) - K @ self.H) @ self.P\n",
    "\n",
    "# === SETTINGS ===\n",
    "MATCH_THRESHOLD = 130\n",
    "NORMALIZED_MATCH_THRESHOLD = 0.3  # for normalized mode\n",
    "USE_NORMALIZED_DISTANCE = True    # <<< SWITCH: True = normalized, False = pixel distance\n",
    "\n",
    "VIDEO_INPUT = \"/Users/Christian/Downloads/Fabrice Videos/test/processed_1c ohne Intervention.MP4\"\n",
    "INPUT_PATH = Path(VIDEO_INPUT)\n",
    "OUTPUT_FOLDER = INPUT_PATH.parent / \"corrected\"\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === LOAD DATA ===\n",
    "tracking_df = pd.read_csv(INPUT_PATH.parent / \"YOLO\" / (INPUT_PATH.stem + \".csv\"))\n",
    "pose_df = pd.read_csv(INPUT_PATH.parent / \"DLC\" / (INPUT_PATH.stem + \"_predictions.csv\"), low_memory=False)\n",
    "\n",
    "multi_index = pd.MultiIndex.from_arrays(pose_df.iloc[0:3].values, names=[\"individual\", \"bodypart\", \"coord\"])\n",
    "pose_data_cleaned = pose_df.iloc[3:].copy()\n",
    "pose_data_cleaned.columns = multi_index\n",
    "pose_data_cleaned.reset_index(drop=True, inplace=True)\n",
    "pose_data_cleaned[\"Frame\"] = pose_data_cleaned.index.astype(int)\n",
    "\n",
    "bodyparts = pose_data_cleaned.columns.get_level_values(\"bodypart\").unique()\n",
    "bodyparts = [bp for bp in bodyparts if bp not in [\"Frame\", \"scorer\"]]\n",
    "\n",
    "# === TRACKING STATE ===\n",
    "kalman_filters = {}\n",
    "previous_ids = set()\n",
    "\n",
    "# === FINAL OUTPUT DATA ===\n",
    "final_data = []\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for frame_idx, pose_row in tqdm(pose_data_cleaned.iterrows(), total=len(pose_data_cleaned), desc=\"Processing Frames\"):\n",
    "    frame_num = int(pose_row[\"Frame\"])\n",
    "    frame_boxes = tracking_df[tracking_df.Frame == frame_num]\n",
    "\n",
    "    detected_ids = set(frame_boxes.Person_ID.values)\n",
    "    individuals = pose_row.index.get_level_values(\"individual\").unique()\n",
    "    individuals = [ind for ind in individuals if ind.startswith(\"idv_\")]\n",
    "\n",
    "    # Prepare pose centroids\n",
    "    pose_centroids = {}\n",
    "    for ind in individuals:\n",
    "        keypoints = []\n",
    "        for bp in bodyparts:\n",
    "            try:\n",
    "                x = float(pose_row[(ind, bp, \"x\")])\n",
    "                y = float(pose_row[(ind, bp, \"y\")])\n",
    "                if pd.notna(x) and pd.notna(y):\n",
    "                    keypoints.append((x, y))\n",
    "            except:\n",
    "                continue\n",
    "        if keypoints:\n",
    "            cx, cy = np.mean(keypoints, axis=0)\n",
    "            pose_centroids[ind] = (cx, cy)\n",
    "\n",
    "    # Detect if reassignment is necessary\n",
    "    lost_ids = previous_ids - detected_ids\n",
    "    new_ids = detected_ids - previous_ids\n",
    "\n",
    "    # Build reassignment map\n",
    "    reassignment_map = {}\n",
    "    tried_reassignment_flags = {}\n",
    "\n",
    "    for new_id in new_ids:\n",
    "        box = frame_boxes[frame_boxes.Person_ID == new_id].iloc[0]\n",
    "        x_center = (box.X1 + box.X2) / 2\n",
    "        y_center = (box.Y1 + box.Y2) / 2\n",
    "\n",
    "        best_match = None\n",
    "        min_distance = float('inf')\n",
    "\n",
    "        for skel_id, (cx, cy) in pose_centroids.items():\n",
    "            # Calculate center distance (always)\n",
    "            center_distance = np.sqrt((cx - x_center)**2 + (cy - y_center)**2)\n",
    "            \n",
    "            # Normalize distance if needed\n",
    "            if USE_NORMALIZED_DISTANCE:\n",
    "                bbox_width = box.X2 - box.X1\n",
    "                bbox_height = box.Y2 - box.Y1\n",
    "                avg_bbox_size = (bbox_width + bbox_height) / 2\n",
    "                if avg_bbox_size > 0:\n",
    "                    center_distance /= avg_bbox_size\n",
    "            \n",
    "            # Check match threshold\n",
    "            threshold = NORMALIZED_MATCH_THRESHOLD if USE_NORMALIZED_DISTANCE else MATCH_THRESHOLD\n",
    "            \n",
    "            if center_distance < min_distance and center_distance < threshold:\n",
    "                best_match = skel_id\n",
    "                min_distance = center_distance\n",
    "\n",
    "\n",
    "        if best_match is not None:\n",
    "            reassignment_map[new_id] = new_id  # keep ID\n",
    "            tried_reassignment_flags[new_id] = 1\n",
    "        else:\n",
    "            reassignment_map[new_id] = new_id  # no match, accept new ID\n",
    "            tried_reassignment_flags[new_id] = 1\n",
    "\n",
    "    # IDs that are consistent (no reassignment needed)\n",
    "    for stable_id in detected_ids.intersection(previous_ids):\n",
    "        reassignment_map[stable_id] = stable_id\n",
    "        tried_reassignment_flags[stable_id] = 0\n",
    "\n",
    "    # === Save frame info ===\n",
    "    for pid in detected_ids:\n",
    "        box = frame_boxes[frame_boxes.Person_ID == pid].iloc[0]\n",
    "        new_id = reassignment_map.get(pid, pid)\n",
    "\n",
    "        data = {\n",
    "            \"Frame\": frame_num,\n",
    "            \"Old_ID\": pid,\n",
    "            \"New_ID\": new_id,\n",
    "            \"Tried_Reassignment\": tried_reassignment_flags.get(pid, 0),\n",
    "            \"X1\": box.X1,\n",
    "            \"Y1\": box.Y1,\n",
    "            \"X2\": box.X2,\n",
    "            \"Y2\": box.Y2,\n",
    "        }\n",
    "\n",
    "        # Attach skeleton if match possible\n",
    "        skel_num = None\n",
    "        for skel_id, (cx, cy) in pose_centroids.items():\n",
    "            center_box = ((box.X1 + box.X2) / 2, (box.Y1 + box.Y2) / 2)\n",
    "            \n",
    "            center_distance = np.linalg.norm(np.array([cx, cy]) - np.array(center_box))\n",
    "        \n",
    "            if USE_NORMALIZED_DISTANCE:\n",
    "                bbox_width = box.X2 - box.X1\n",
    "                bbox_height = box.Y2 - box.Y1\n",
    "                avg_bbox_size = (bbox_width + bbox_height) / 2\n",
    "                if avg_bbox_size > 0:\n",
    "                    center_distance /= avg_bbox_size\n",
    "        \n",
    "            threshold = NORMALIZED_MATCH_THRESHOLD if USE_NORMALIZED_DISTANCE else MATCH_THRESHOLD\n",
    "        \n",
    "            if center_distance < threshold:\n",
    "                skel_num = skel_id.split(\"_\")[1]\n",
    "                break\n",
    "        \n",
    "\n",
    "        if skel_num:\n",
    "            for bp in bodyparts:\n",
    "                x = pose_row.get((f\"idv_{skel_num}\", bp, \"x\"), np.nan)\n",
    "                y = pose_row.get((f\"idv_{skel_num}\", bp, \"y\"), np.nan)\n",
    "                conf = pose_row.get((f\"idv_{skel_num}\", bp, \"likelihood\"), np.nan)\n",
    "                data[f\"{bp}_x\"] = x\n",
    "                data[f\"{bp}_y\"] = y\n",
    "                data[f\"{bp}_conf\"] = conf\n",
    "\n",
    "        final_data.append(data)\n",
    "\n",
    "    previous_ids = detected_ids.copy()\n",
    "\n",
    "# === FINALIZE ===\n",
    "final_df = pd.DataFrame(final_data)\n",
    "final_df = final_df.sort_values([\"Frame\", \"Old_ID\"])\n",
    "final_df.to_csv(OUTPUT_FOLDER / \"final_corrected_output_stable_with_old_new_and_likelihoods.csv\", index=False)\n",
    "\n",
    "print(\"\\u2705 Final corrected CSV with Old/New IDs, reassignment flags, and body markers saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652c0d8-f550-4400-94b0-ae015965e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassignment based on YOLO, FastReID, and RTM: \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === SETTINGS ===\n",
    "MATCH_THRESHOLD = 100\n",
    "MOVEMENT_THRESHOLD = 50\n",
    "VIDEO_INPUT = \"/Users/Christian/Downloads/Fabrice Videos/test/processed_1c ohne Intervention.MP4\"\n",
    "input_video_path = Path(VIDEO_INPUT)\n",
    "OUTPUT_FOLDER = input_video_path.parent / \"corrected\"\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "bodypart_colors = {\n",
    "    \"nose\": (255, 0, 0),       # red\n",
    "    \"left_eye\": (0, 255, 0),    # green\n",
    "    \"right_eye\": (0, 0, 255),   # blue\n",
    "    \"left_ear\": (255, 255, 0),  # cyan\n",
    "    \"right_ear\": (255, 0, 255), # magenta\n",
    "    \"left_shoulder\": (0, 255, 255),\n",
    "    \"right_shoulder\": (255, 128, 0),\n",
    "    \"left_elbow\": (128, 0, 255),\n",
    "    \"right_elbow\": (0, 128, 255),\n",
    "    \"left_wrist\": (128, 128, 0),\n",
    "    \"right_wrist\": (0, 128, 128),\n",
    "    \"left_hip\": (255, 128, 128),\n",
    "    \"right_hip\": (128, 255, 128),\n",
    "    \"left_knee\": (128, 128, 255),\n",
    "    \"right_knee\": (255, 255, 128),\n",
    "    \"left_ankle\": (255, 128, 255),\n",
    "    \"right_ankle\": (128, 255, 255),\n",
    "}\n",
    "\n",
    "# === STEP 1: Load data ===\n",
    "tracking_df = pd.read_csv(\"/Users/Christian/Downloads/Fabrice Videos/test/YOLO/processed_1c ohne Intervention.csv\")\n",
    "pose_df = pd.read_csv(\"/Users/Christian/Downloads/Fabrice Videos/test/DLC/processed_1c ohne Intervention_predictions.csv\", low_memory=False)\n",
    "\n",
    "# === STEP 2: Prepare pose data ===\n",
    "multi_index = pd.MultiIndex.from_arrays(pose_df.iloc[0:3].values, names=[\"individual\", \"bodypart\", \"coord\"])\n",
    "pose_data_cleaned = pose_df.iloc[3:].copy()\n",
    "pose_data_cleaned.columns = multi_index\n",
    "pose_data_cleaned.reset_index(drop=True, inplace=True)\n",
    "pose_data_cleaned[\"Frame\"] = pose_data_cleaned.index.astype(int)\n",
    "\n",
    "bodyparts = pose_data_cleaned.columns.get_level_values(\"bodypart\").unique()\n",
    "bodyparts = [bp for bp in bodyparts if bp not in [\"Frame\", \"scorer\"]]\n",
    "\n",
    "# === STEP 3: Match skeletons to bounding boxes ===\n",
    "tracking_by_frame = tracking_df.groupby(\"Frame\")\n",
    "enriched_matches = []\n",
    "\n",
    "for frame_idx, row in tqdm(pose_data_cleaned.iterrows(), total=len(pose_data_cleaned), desc=\"Matching Skeletons\"):\n",
    "    frame_num = int(row[\"Frame\"].iloc[0])\n",
    "    if frame_num not in tracking_by_frame.groups:\n",
    "        continue\n",
    "\n",
    "    frame_boxes = tracking_by_frame.get_group(frame_num)\n",
    "    individuals = pose_data_cleaned.columns.get_level_values(\"individual\").unique()\n",
    "    individuals = [ind for ind in individuals if ind.startswith(\"idv_\")]\n",
    "\n",
    "    skeleton_centroids = []\n",
    "    skeleton_ids = []\n",
    "\n",
    "    for ind in individuals:\n",
    "        keypoints = []\n",
    "        for bp in bodyparts:\n",
    "            try:\n",
    "                x = float(row[(ind, bp, \"x\")])\n",
    "                y = float(row[(ind, bp, \"y\")])\n",
    "                if pd.notna(x) and pd.notna(y):\n",
    "                    keypoints.append((x, y))\n",
    "            except (ValueError, KeyError, TypeError):\n",
    "                continue\n",
    "\n",
    "        if not keypoints:\n",
    "            continue\n",
    "\n",
    "        cx, cy = np.mean(keypoints, axis=0)\n",
    "        skeleton_centroids.append((cx, cy))\n",
    "        skeleton_ids.append(ind)\n",
    "\n",
    "    if not skeleton_centroids:\n",
    "        continue\n",
    "\n",
    "    skeleton_centroids = np.array(skeleton_centroids)\n",
    "    box_centers = np.column_stack(((frame_boxes.X1 + frame_boxes.X2) / 2, (frame_boxes.Y1 + frame_boxes.Y2) / 2))\n",
    "    distances = cdist(skeleton_centroids, box_centers)\n",
    "\n",
    "    for sk_idx, sk_id in enumerate(skeleton_ids):\n",
    "        min_dist = np.min(distances[sk_idx])\n",
    "        min_idx = np.argmin(distances[sk_idx])\n",
    "        if min_dist < MATCH_THRESHOLD:\n",
    "            matched_person_id = frame_boxes.iloc[min_idx].Person_ID\n",
    "            enriched_matches.append({\n",
    "                \"Frame\": frame_num,\n",
    "                \"Skeleton_ID\": sk_id,\n",
    "                \"Centroid_X\": skeleton_centroids[sk_idx][0],\n",
    "                \"Centroid_Y\": skeleton_centroids[sk_idx][1],\n",
    "                \"Matched_Person_ID\": matched_person_id,\n",
    "                \"Distance\": min_dist\n",
    "            })\n",
    "\n",
    "enriched_df = pd.DataFrame(enriched_matches)\n",
    "\n",
    "# === STEP 4: Track and Correct Skeleton Trajectories ===\n",
    "skeleton_tracks = {}\n",
    "for sk_id, group in enriched_df.groupby(\"Skeleton_ID\"):\n",
    "    trajectory = group.sort_values(\"Frame\")[[\"Frame\", \"Centroid_X\", \"Centroid_Y\", \"Matched_Person_ID\"]].reset_index(drop=True)\n",
    "    skeleton_tracks[sk_id] = trajectory\n",
    "\n",
    "corrections = []\n",
    "for sk_id, df in skeleton_tracks.items():\n",
    "    previous_id = None\n",
    "    for idx, row in df.iterrows():\n",
    "        current_id = row[\"Matched_Person_ID\"]\n",
    "        frame = row[\"Frame\"]\n",
    "        if previous_id is not None and current_id != previous_id:\n",
    "            prev_row = df.iloc[idx - 1]\n",
    "            dx = row[\"Centroid_X\"] - prev_row[\"Centroid_X\"]\n",
    "            dy = row[\"Centroid_Y\"] - prev_row[\"Centroid_Y\"]\n",
    "            movement = np.sqrt(dx ** 2 + dy ** 2)\n",
    "            if movement < 50:\n",
    "                corrections.append({\n",
    "                    \"Frame\": frame,\n",
    "                    \"Skeleton_ID\": sk_id,\n",
    "                    \"Old_ID\": current_id,\n",
    "                    \"New_ID\": previous_id\n",
    "                })\n",
    "                df.at[idx, \"Matched_Person_ID\"] = previous_id\n",
    "        previous_id = df.at[idx, \"Matched_Person_ID\"]\n",
    "\n",
    "corrected_df = pd.concat(skeleton_tracks.values(), ignore_index=True)\n",
    "\n",
    "# Add bounding box info\n",
    "corrected_with_boxes = pd.merge(\n",
    "    corrected_df,\n",
    "    tracking_df.rename(columns={\"Person_ID\": \"Matched_Person_ID\"}),\n",
    "    on=[\"Frame\", \"Matched_Person_ID\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Flatten pose markers\n",
    "pose_export = pose_data_cleaned.copy()\n",
    "pose_export_frame = pose_export[\"Frame\"]\n",
    "pose_export.columns = ['_'.join(col).strip() for col in pose_export.columns.values]\n",
    "pose_export[\"Frame\"] = pose_export_frame  # restore usable 'Frame' key\n",
    "final_output = pd.merge(corrected_with_boxes, pose_export, on=\"Frame\", how=\"left\")\n",
    "\n",
    "final_output.to_csv(Path(OUTPUT_FOLDER) / \"corrected_with_boxes_and_pose.csv\", index=False)\n",
    "\n",
    "# === STEP 5: Create Visual Inspection Video ===\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "out_video = cv2.VideoWriter(str(Path(OUTPUT_FOLDER) / \"visual_inspection.mp4\"),\n",
    "                            cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "colors = {}\n",
    "print(\"Creating video...\")\n",
    "\n",
    "for frame_idx in tqdm(range(total_frames), desc=\"Creating Video\"):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_matches = corrected_df[corrected_df.Frame == frame_idx]\n",
    "    frame_boxes = tracking_df[tracking_df.Frame == frame_idx]\n",
    "    pose_row = pose_data_cleaned.loc[pose_data_cleaned[\"Frame\"] == frame_idx]\n",
    "\n",
    "    for _, row in frame_matches.iterrows():\n",
    "        pid = int(row[\"Matched_Person_ID\"])\n",
    "        cx, cy = int(row[\"Centroid_X\"]), int(row[\"Centroid_Y\"])\n",
    "        color = colors.setdefault(pid, tuple(np.random.randint(0, 255, size=3).tolist()))\n",
    "        color = bodypart_colors.get(bp, (255, 255, 0))  # fallback to yellow\n",
    "        cv2.circle(frame, (int(x), int(y)), 5, color, -1)\n",
    "        cv2.putText(frame, f'ID {pid}', (cx + 10, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 3)\n",
    "\n",
    "    for _, box in frame_boxes.iterrows():\n",
    "        x1, y1, x2, y2 = int(box.X1), int(box.Y1), int(box.X2), int(box.Y2)\n",
    "        pid = int(box.Person_ID)\n",
    "        color = colors.setdefault(pid, tuple(np.random.randint(0, 255, size=3).tolist()))\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "    # draw all bodymarkers per person\n",
    "    if not pose_row.empty:\n",
    "        individuals = pose_row.columns.get_level_values(\"individual\").unique()\n",
    "        individuals = [ind for ind in individuals if ind.startswith(\"idv_\")]\n",
    "        for ind in individuals:\n",
    "            for bp in bodyparts:\n",
    "                try:\n",
    "                    x = float(pose_row[(ind, bp, \"x\")].values[0])\n",
    "                    y = float(pose_row[(ind, bp, \"y\")].values[0])\n",
    "                    if pd.notna(x) and pd.notna(y):\n",
    "                        cv2.circle(frame, (int(x), int(y)), 3, (255, 255, 0), -1)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    out_video.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out_video.release()\n",
    "print(\"‚úÖ Visual inspection video created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402372d4-f280-4950-a1bb-2769151abbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLC and RMT Pose:\n",
    "# https://huggingface.co/DeepLabCut/HumanBody/tree/main\n",
    "\n",
    "from pathlib import Path\n",
    "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
    "from deeplabcut.utils.video_processor import VideoProcessorCV\n",
    "from deeplabcut.utils.make_labeled_video import CreateVideo\n",
    "import cv2\n",
    "import torchvision.models.detection as detection\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "# === INPUT & OUTPUT PATHS ===\n",
    "video_folder = Path(\"/Users/Christian/Downloads/Fabrice Videos/test\")  # Folder with input .mp4 videos\n",
    "output_dir = Path(\"/Users/Christian/Downloads/Fabrice Videos/test/DLC\")    # Folder to save outputs\n",
    "\n",
    "# Clean and recreate output folder\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "output_dir.mkdir(parents=True)\n",
    "\n",
    "# Load all video files\n",
    "video_files = list(video_folder.glob(\"*.mp4\")) + list(video_folder.glob(\"*.MP4\")) + list(video_folder.glob(\"*.Mp4\")) + list(video_folder.glob(\"*.mP4\"))\n",
    "\n",
    "# === Model Configuration Paths ===\n",
    "path_model_config = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7_pytorch_config.yaml\")\n",
    "path_snapshot = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7.pt\")\n",
    "\n",
    "# === Inference Settings ===\n",
    "device = \"cpu\"  # change to \"cuda\" if using GPU\n",
    "max_detections = 20\n",
    "\n",
    "# === Load Object Detector ===\n",
    "weights = detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "detector = detection.fasterrcnn_mobilenet_v3_large_fpn(weights=weights, box_score_thresh=0.6)\n",
    "detector.eval().to(device)\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# === Load RTMPose Model ===\n",
    "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
    "runner = dlc_torch.get_pose_inference_runner(\n",
    "    pose_cfg,\n",
    "    snapshot_path=path_snapshot,\n",
    "    batch_size=4,\n",
    "    max_individuals=max_detections,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# === Process Each Video ===\n",
    "for video_path in tqdm(video_files, desc=\"Processing videos\"):\n",
    "    print(f\"\\nüìπ Processing: {video_path.name}\")\n",
    "\n",
    "    output_csv_path = output_dir / f\"{video_path.stem}_predictions.csv\"\n",
    "    output_video_path = output_dir / f\"{video_path.stem}_labeled.mp4\"\n",
    "\n",
    "    # === Extract Frames ===\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    # === Run Object Detection ===\n",
    "    context = []\n",
    "    with torch.no_grad():\n",
    "        for frame in tqdm(frames, desc=\"Running object detection\", leave=False):\n",
    "            image = Image.fromarray(frame).convert(\"RGB\")\n",
    "            batch = [preprocess(image).to(device)]\n",
    "            predictions = detector(batch)[0]\n",
    "            bboxes = predictions[\"boxes\"].cpu().numpy()\n",
    "            labels = predictions[\"labels\"].cpu().numpy()\n",
    "\n",
    "            # Keep only 'person' detections (label 1)\n",
    "            human_bboxes = [bbox for bbox, label in zip(bboxes, labels) if label == 1]\n",
    "            if human_bboxes:\n",
    "                # Use largest bbox (by area)\n",
    "                human_bboxes = sorted(human_bboxes, key=lambda b: (b[2]-b[0])*(b[3]-b[1]), reverse=True)\n",
    "                bbox = human_bboxes[0]\n",
    "                bbox[2] -= bbox[0]  # convert to width\n",
    "                bbox[3] -= bbox[1]  # convert to height\n",
    "                bboxes = np.array([bbox])\n",
    "            else:\n",
    "                bboxes = np.zeros((0, 4))\n",
    "\n",
    "            context.append({\"bboxes\": bboxes})\n",
    "\n",
    "    # === Run RTMPose Inference ===\n",
    "    frames_with_context = list(zip(frames, context))\n",
    "    predictions = runner.inference(tqdm(frames_with_context, desc=\"Running RTMPose\", leave=False))\n",
    "\n",
    "    # === Save Predictions ===\n",
    "    df = dlc_torch.build_predictions_dataframe(\n",
    "        scorer=\"rtmpose-body7\",\n",
    "        predictions={i: pred for i, pred in enumerate(predictions)},\n",
    "        parameters=dlc_torch.PoseDatasetParameters(\n",
    "            bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "            unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "            individuals=[f\"idv_{i}\" for i in range(max_detections)]\n",
    "        )\n",
    "    )\n",
    "    df.to_csv(output_csv_path)\n",
    "    print(f\"‚úÖ Saved predictions: {output_csv_path.name}\")\n",
    "\n",
    "    # === Create Labeled Video ===\n",
    "    clip = VideoProcessorCV(str(video_path), sname=str(output_video_path), codec=\"mp4v\")\n",
    "    CreateVideo(\n",
    "        clip,\n",
    "        df,\n",
    "        pcutoff=0.4,\n",
    "        dotsize=3,\n",
    "        colormap=\"rainbow\",\n",
    "        bodyparts2plot=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "        trailpoints=0,\n",
    "        cropping=False,\n",
    "        x1=0,\n",
    "        x2=clip.w,\n",
    "        y1=0,\n",
    "        y2=clip.h,\n",
    "        bodyparts2connect=[\n",
    "            [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "            [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "            [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "            [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "        ],\n",
    "        skeleton_color=\"k\",\n",
    "        draw_skeleton=True,\n",
    "        displaycropped=False,\n",
    "        color_by=\"bodypart\",\n",
    "    )\n",
    "    print(f\"üé¨ Labeled video saved: {output_video_path.name}\")\n",
    "\n",
    "    # === Clear memory\n",
    "    del frames\n",
    "    del context\n",
    "    del frames_with_context\n",
    "    del predictions\n",
    "    del df\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n‚úÖ All videos processed and saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af43e72-4e54-4f54-bbb5-a36de096ded3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc60096-3954-424b-8547-e318d98dae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTM Pose only for Pictures:\n",
    "# https://huggingface.co/DeepLabCut/HumanBody/tree/main\n",
    "\n",
    "from pathlib import Path\n",
    "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import torchvision.models.detection as detection\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === Absolute paths to your files ===\n",
    "image_folder = Path(\"/Users/Christian/rtm_pose/images\")\n",
    "path_model_config = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7_pytorch_config.yaml\")\n",
    "path_snapshot = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7.pt\")\n",
    "output_csv_path = Path(\"/Users/Christian/rtm_pose/image_predictions.csv\")\n",
    "\n",
    "# === Device and config ===\n",
    "device = \"cpu\"  # Use \"mps\" if you want to try the Apple GPU\n",
    "max_detections = 1\n",
    "\n",
    "# === Load object detector ===\n",
    "weights = detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "detector = detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    weights=weights,\n",
    "    box_score_thresh=0.6,\n",
    ")\n",
    "detector.eval().to(device)\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# === Detect people in images ===\n",
    "image_paths = sorted(image_folder.glob(\"*.jpg\"))\n",
    "context = []\n",
    "\n",
    "print(\"Running object detection...\")\n",
    "with torch.no_grad():\n",
    "    for image_path in tqdm(image_paths, leave=True):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        batch = [preprocess(image).to(device)]\n",
    "        predictions = detector(batch)[0]\n",
    "        bboxes = predictions[\"boxes\"].cpu().numpy()\n",
    "        labels = predictions[\"labels\"].cpu().numpy()\n",
    "\n",
    "        human_bboxes = [bbox for bbox, label in zip(bboxes, labels) if label == 1]\n",
    "\n",
    "        bboxes = np.zeros((0, 4))\n",
    "        if human_bboxes:\n",
    "            bboxes = np.stack(human_bboxes)\n",
    "        bboxes[:, 2] -= bboxes[:, 0]\n",
    "        bboxes[:, 3] -= bboxes[:, 1]\n",
    "        bboxes = bboxes[:max_detections]\n",
    "\n",
    "        context.append({\"bboxes\": bboxes})\n",
    "\n",
    "# === Run RTMPose inference ===\n",
    "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
    "runner = dlc_torch.get_pose_inference_runner(\n",
    "    pose_cfg,\n",
    "    snapshot_path=path_snapshot,\n",
    "    batch_size=4,\n",
    "    max_individuals=max_detections,\n",
    ")\n",
    "\n",
    "print(\"Running pose estimation...\")\n",
    "predictions = runner.inference(zip(image_paths, context))\n",
    "\n",
    "# === Save predictions to CSV ===\n",
    "print(\"Saving predictions...\")\n",
    "df = dlc_torch.build_predictions_dataframe(\n",
    "    scorer=\"rtmpose-body7\",\n",
    "    predictions={str(p): pred for p, pred in zip(image_paths, predictions)},\n",
    "    parameters=dlc_torch.PoseDatasetParameters(\n",
    "        bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "        unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "        individuals=[f\"idv_{i}\" for i in range(max_detections)]\n",
    "    )\n",
    ")\n",
    "df.to_csv(output_csv_path)\n",
    "print(f\"‚úÖ Done! Predictions saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ebfaf-8b0d-47df-8e56-7ed1a6a06f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labelled pictures: \n",
    "\n",
    "# === Skeleton definition ===\n",
    "skeleton = [\n",
    "    [16, 14], [14, 12], [17, 15], [15, 13],\n",
    "    [12, 13], [6, 12], [7, 13], [6, 7],\n",
    "    [6, 8], [7, 9], [8, 10], [9, 11],\n",
    "    [2, 3], [1, 2], [1, 3], [2, 4],\n",
    "    [3, 5], [4, 6], [5, 7],\n",
    "]\n",
    "\n",
    "# === Plot options ===\n",
    "plot_skeleton = True\n",
    "plot_pose_markers = True\n",
    "plot_bounding_boxes = True\n",
    "marker_size = 12\n",
    "cmap_keypoints = plt.get_cmap(\"rainbow\")\n",
    "cmap_skeleton = plt.get_cmap(\"rainbow\")\n",
    "\n",
    "# === Folder to save visualizations ===\n",
    "output_dir = Path(\"/Users/Christian/rtm_pose/labeled_images\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Loop through and save images ===\n",
    "for image_path, image_predictions in zip(image_paths, predictions):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pose = image_predictions[\"bodyparts\"]\n",
    "    bboxes = image_predictions[\"bboxes\"]\n",
    "    num_individuals, num_bodyparts = pose.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "    ax.set_xlim(0, image.width)\n",
    "    ax.set_ylim(image.height, 0)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    for idv_pose in pose:\n",
    "        if plot_skeleton:\n",
    "            bones = []\n",
    "            for bpt_1, bpt_2 in skeleton:\n",
    "                bones.append([idv_pose[bpt_1 - 1, :2], idv_pose[bpt_2 - 1, :2]])\n",
    "            bone_colors = cmap_skeleton(np.linspace(0, 1, len(skeleton)))\n",
    "            ax.add_collection(collections.LineCollection(bones, colors=bone_colors))\n",
    "\n",
    "        if plot_pose_markers:\n",
    "            ax.scatter(\n",
    "                idv_pose[:, 0],\n",
    "                idv_pose[:, 1],\n",
    "                c=list(range(num_bodyparts)),\n",
    "                cmap=cmap_keypoints,\n",
    "                s=marker_size,\n",
    "            )\n",
    "\n",
    "    if plot_bounding_boxes:\n",
    "        for x, y, w, h in bboxes:\n",
    "            ax.plot([x, x + w, x + w, x, x], [y, y, y + h, y + h, y], c=\"red\")\n",
    "\n",
    "    # === Save image ===\n",
    "    output_file = output_dir / f\"{image_path.stem}_labeled.jpg\"\n",
    "    plt.savefig(output_file, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "    plt.close(fig)  # Close the figure to avoid memory buildup\n",
    "\n",
    "    print(f\"‚úÖ Saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf0f31-7870-470f-8621-6ee6e91966ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show labelled pictures:\n",
    "\n",
    "# === Skeleton definition (same as in the model config) ===\n",
    "skeleton = [\n",
    "    [16, 14], [14, 12], [17, 15], [15, 13],\n",
    "    [12, 13], [6, 12], [7, 13], [6, 7],\n",
    "    [6, 8], [7, 9], [8, 10], [9, 11],\n",
    "    [2, 3], [1, 2], [1, 3], [2, 4],\n",
    "    [3, 5], [4, 6], [5, 7],\n",
    "]\n",
    "\n",
    "# === Plot options ===\n",
    "plot_skeleton = True\n",
    "plot_pose_markers = True\n",
    "plot_bounding_boxes = True\n",
    "marker_size = 12\n",
    "cmap_keypoints = plt.get_cmap(\"rainbow\")\n",
    "cmap_skeleton = plt.get_cmap(\"rainbow\")\n",
    "\n",
    "# === Loop through images and predictions ===\n",
    "for image_path, image_predictions in zip(image_paths, predictions):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    pose = image_predictions[\"bodyparts\"]\n",
    "    bboxes = image_predictions[\"bboxes\"]\n",
    "    num_individuals, num_bodyparts = pose.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "    ax.set_xlim(0, image.width)\n",
    "    ax.set_ylim(image.height, 0)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    for idv_pose in pose:\n",
    "        if plot_skeleton:\n",
    "            bones = []\n",
    "            for bpt_1, bpt_2 in skeleton:\n",
    "                bones.append([idv_pose[bpt_1 - 1, :2], idv_pose[bpt_2 - 1, :2]])\n",
    "\n",
    "            bone_colors = cmap_skeleton(np.linspace(0, 1, len(skeleton)))\n",
    "            ax.add_collection(collections.LineCollection(bones, colors=bone_colors))\n",
    "\n",
    "        if plot_pose_markers:\n",
    "            ax.scatter(\n",
    "                idv_pose[:, 0],\n",
    "                idv_pose[:, 1],\n",
    "                c=list(range(num_bodyparts)),\n",
    "                cmap=cmap_keypoints,\n",
    "                s=marker_size,\n",
    "            )\n",
    "\n",
    "    if plot_bounding_boxes:\n",
    "        for x, y, w, h in bboxes:\n",
    "            ax.plot([x, x + w, x + w, x, x], [y, y, y + h, y + h, y], c=\"red\")\n",
    "\n",
    "    plt.title(image_path.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb430097-079d-4764-bc51-c22491a585f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: REASSIGNMENT === UPDATED SCRIPT: Add Full Skeleton + Kalman Filter ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === SIMPLE KALMAN FILTER CLASS ===\n",
    "class SimpleKalman2D:\n",
    "    def __init__(self, x, y):\n",
    "        self.state = np.array([x, y, 0, 0], dtype=float)\n",
    "        self.P = np.eye(4) * 1000\n",
    "        self.F = np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]])\n",
    "        self.H = np.array([[1,0,0,0],[0,1,0,0]])\n",
    "        self.R = np.eye(2) * 10\n",
    "        self.Q = np.eye(4) * 0.01\n",
    "\n",
    "    def predict(self):\n",
    "        self.state = self.F @ self.state\n",
    "        self.P = self.F @ self.P @ self.F.T + self.Q\n",
    "        return self.state[0], self.state[1]\n",
    "\n",
    "    def update(self, x, y):\n",
    "        z = np.array([x, y])\n",
    "        y_ = z - (self.H @ self.state)\n",
    "        S = self.H @ self.P @ self.H.T + self.R\n",
    "        K = self.P @ self.H.T @ np.linalg.inv(S)\n",
    "        self.state += K @ y_\n",
    "        self.P = (np.eye(4) - K @ self.H) @ self.P\n",
    "\n",
    "# === SETTINGS ===\n",
    "MATCH_THRESHOLD = 100\n",
    "MOVEMENT_THRESHOLD = 50\n",
    "VIDEO_INPUT = \"/Users/Christian/Downloads/Fabrice Videos/test/processed_1c ohne Intervention.MP4\"\n",
    "input_video_path = Path(VIDEO_INPUT)\n",
    "OUTPUT_FOLDER = input_video_path.parent / \"corrected\"\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kalman_filters = {}\n",
    "\n",
    "# === BODY PART COLORS ===\n",
    "bodypart_colors = {\n",
    "    \"nose\": (255, 0, 0), \"left_eye\": (0, 255, 0), \"right_eye\": (0, 0, 255),\n",
    "    \"left_ear\": (255, 255, 0), \"right_ear\": (255, 0, 255),\n",
    "    \"left_shoulder\": (0, 255, 255), \"right_shoulder\": (255, 128, 0),\n",
    "    \"left_elbow\": (128, 0, 255), \"right_elbow\": (0, 128, 255),\n",
    "    \"left_wrist\": (128, 128, 0), \"right_wrist\": (0, 128, 128),\n",
    "    \"left_hip\": (255, 128, 128), \"right_hip\": (128, 255, 128),\n",
    "    \"left_knee\": (128, 128, 255), \"right_knee\": (255, 255, 128),\n",
    "    \"left_ankle\": (255, 128, 255), \"right_ankle\": (128, 255, 255),\n",
    "}\n",
    "\n",
    "# === LOAD DATA ===\n",
    "tracking_df = pd.read_csv(input_video_path.parent / \"YOLO\" / (input_video_path.stem + \".csv\"))\n",
    "pose_df = pd.read_csv(input_video_path.parent / \"DLC\" / (input_video_path.stem + \"_predictions.csv\"), low_memory=False)\n",
    "\n",
    "multi_index = pd.MultiIndex.from_arrays(pose_df.iloc[0:3].values, names=[\"individual\", \"bodypart\", \"coord\"])\n",
    "pose_data_cleaned = pose_df.iloc[3:].copy()\n",
    "pose_data_cleaned.columns = multi_index\n",
    "pose_data_cleaned.reset_index(drop=True, inplace=True)\n",
    "pose_data_cleaned[\"Frame\"] = pose_data_cleaned.index.astype(int)\n",
    "\n",
    "bodyparts = pose_data_cleaned.columns.get_level_values(\"bodypart\").unique()\n",
    "bodyparts = [bp for bp in bodyparts if bp not in [\"Frame\", \"scorer\"]]\n",
    "\n",
    "# === MATCH SKELETONS TO BOXES ===\n",
    "tracking_by_frame = tracking_df.groupby(\"Frame\")\n",
    "enriched_matches = []\n",
    "\n",
    "for frame_idx, row in tqdm(pose_data_cleaned.iterrows(), total=len(pose_data_cleaned), desc=\"Matching Skeletons\"):\n",
    "    frame_num = int(row[\"Frame\"].item())\n",
    "    if frame_num not in tracking_by_frame.groups:\n",
    "        continue\n",
    "\n",
    "    frame_boxes = tracking_by_frame.get_group(frame_num)\n",
    "    individuals = pose_data_cleaned.columns.get_level_values(\"individual\").unique()\n",
    "    individuals = [ind for ind in individuals if ind.startswith(\"idv_\")]\n",
    "\n",
    "    skeleton_centroids = []\n",
    "    skeleton_ids = []\n",
    "\n",
    "    for ind in individuals:\n",
    "        keypoints = []\n",
    "        for bp in bodyparts:\n",
    "            try:\n",
    "                x = float(row[(ind, bp, \"x\")])\n",
    "                y = float(row[(ind, bp, \"y\")])\n",
    "                if pd.notna(x) and pd.notna(y):\n",
    "                    keypoints.append((x, y))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if not keypoints:\n",
    "            continue\n",
    "\n",
    "        cx, cy = np.mean(keypoints, axis=0)\n",
    "\n",
    "        # Kalman filter tracking\n",
    "        if ind not in kalman_filters:\n",
    "            kalman_filters[ind] = SimpleKalman2D(cx, cy)\n",
    "        else:\n",
    "            kalman_filters[ind].update(cx, cy)\n",
    "            cx, cy = kalman_filters[ind].predict()\n",
    "\n",
    "        skeleton_centroids.append((cx, cy))\n",
    "        skeleton_ids.append(ind)\n",
    "\n",
    "    if not skeleton_centroids:\n",
    "        continue\n",
    "\n",
    "    skeleton_centroids = np.array(skeleton_centroids)\n",
    "    box_centers = np.column_stack(((frame_boxes.X1 + frame_boxes.X2) / 2, (frame_boxes.Y1 + frame_boxes.Y2) / 2))\n",
    "    distances = cdist(skeleton_centroids, box_centers)\n",
    "\n",
    "    for sk_idx, sk_id in enumerate(skeleton_ids):\n",
    "        min_dist = np.min(distances[sk_idx])\n",
    "        min_idx = np.argmin(distances[sk_idx])\n",
    "        if min_dist < MATCH_THRESHOLD:\n",
    "            matched_person_id = frame_boxes.iloc[min_idx].Person_ID\n",
    "            enriched_matches.append({\n",
    "                \"Frame\": frame_num,\n",
    "                \"Skeleton_ID\": sk_id,\n",
    "                \"Centroid_X\": skeleton_centroids[sk_idx][0],\n",
    "                \"Centroid_Y\": skeleton_centroids[sk_idx][1],\n",
    "                \"Matched_Person_ID\": matched_person_id,\n",
    "                \"Distance\": min_dist\n",
    "            })\n",
    "\n",
    "# === CORRECTION ===\n",
    "enriched_df = pd.DataFrame(enriched_matches)\n",
    "skeleton_tracks = {}\n",
    "for sk_id, group in enriched_df.groupby(\"Skeleton_ID\"):\n",
    "    trajectory = group.sort_values(\"Frame\")[[\"Frame\", \"Centroid_X\", \"Centroid_Y\", \"Matched_Person_ID\"]].reset_index(drop=True)\n",
    "    skeleton_tracks[sk_id] = trajectory\n",
    "\n",
    "for sk_id, df in skeleton_tracks.items():\n",
    "    previous_id = None\n",
    "    for idx, row in df.iterrows():\n",
    "        current_id = row[\"Matched_Person_ID\"]\n",
    "        frame = row[\"Frame\"]\n",
    "        if previous_id is not None and current_id != previous_id:\n",
    "            prev_row = df.iloc[idx - 1]\n",
    "            dx = row[\"Centroid_X\"] - prev_row[\"Centroid_X\"]\n",
    "            dy = row[\"Centroid_Y\"] - prev_row[\"Centroid_Y\"]\n",
    "            movement = np.sqrt(dx ** 2 + dy ** 2)\n",
    "            if movement < MOVEMENT_THRESHOLD:\n",
    "                df.at[idx, \"Matched_Person_ID\"] = previous_id\n",
    "        previous_id = df.at[idx, \"Matched_Person_ID\"]\n",
    "\n",
    "corrected_df = pd.concat(skeleton_tracks.values(), ignore_index=True)\n",
    "\n",
    "# === MERGE WITH POSE & TRACKING ===\n",
    "corrected_with_boxes = pd.merge(\n",
    "    enriched_df,\n",
    "    tracking_df.rename(columns={\"Person_ID\": \"Matched_Person_ID\"}),\n",
    "    on=[\"Frame\", \"Matched_Person_ID\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "pose_export = pose_data_cleaned.copy()\n",
    "pose_export_frame = pose_export[\"Frame\"]\n",
    "pose_export.columns = ['_'.join(col).strip() for col in pose_export.columns.values]\n",
    "pose_export[\"Frame\"] = pose_export_frame\n",
    "final_output = pd.merge(corrected_with_boxes, pose_export, on=\"Frame\", how=\"left\")\n",
    "final_output.to_csv(Path(OUTPUT_FOLDER) / \"corrected_with_boxes_and_pose.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Correction complete with full skeleton and Kalman filtering.\")\n",
    "\n",
    "\n",
    "\n",
    "# === Create Visual Inspection Video ===\n",
    "create_labeled_video = False  # Set this to False if you don't want labeled videos\n",
    "\n",
    "# === Optional: Create labeled video if needed\n",
    "if create_labeled_video:\n",
    "    cap = cv2.VideoCapture(str(VIDEO_INPUT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    out_video = cv2.VideoWriter(\n",
    "        str(OUTPUT_FOLDER / \"visual_inspection.mp4\"),\n",
    "        cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "        fps,\n",
    "        (frame_width, frame_height)\n",
    "    )\n",
    "    \n",
    "    colors = {}\n",
    "    print(\"üé• Creating visualization video...\")\n",
    "    \n",
    "    progress_bar = tqdm(total=total_frames, desc=\"Creating Video\", unit=\"frame\", ncols=300, leave=True)\n",
    "    \n",
    "    for frame_idx in tqdm(range(total_frames), desc=\"Creating Video\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        frame_matches = corrected_df[corrected_df.Frame == frame_idx]\n",
    "        frame_boxes = tracking_df[tracking_df.Frame == frame_idx]\n",
    "        pose_row = pose_data_cleaned.loc[pose_data_cleaned[\"Frame\"] == frame_idx]\n",
    "    \n",
    "        # Draw bounding boxes and IDs\n",
    "        for _, box in frame_boxes.iterrows():\n",
    "            x1, y1, x2, y2 = int(box.X1), int(box.Y1), int(box.X2), int(box.Y2)\n",
    "            pid = int(box.Person_ID)\n",
    "            color = colors.setdefault(pid, tuple(np.random.randint(0, 255, size=3).tolist()))\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "    \n",
    "        # Draw skeletons and IDs\n",
    "        if not pose_row.empty:\n",
    "            individuals = pose_row.columns.get_level_values(\"individual\").unique()\n",
    "            individuals = [ind for ind in individuals if ind.startswith(\"idv_\")]\n",
    "            for ind in individuals:\n",
    "                keypoints = {}\n",
    "                for bp in bodyparts:\n",
    "                    try:\n",
    "                        x = float(pose_row[(ind, bp, \"x\")].values[0])\n",
    "                        y = float(pose_row[(ind, bp, \"y\")].values[0])\n",
    "                        if pd.notna(x) and pd.notna(y):\n",
    "                            keypoints[bp] = (int(x), int(y))\n",
    "                    except:\n",
    "                        continue\n",
    "    \n",
    "                # Draw joints\n",
    "                for bp, (x, y) in keypoints.items():\n",
    "                    color = bodypart_colors.get(bp, (255, 255, 0))\n",
    "                    cv2.circle(frame, (x, y), 5, color, -1)\n",
    "    \n",
    "                # Draw skeleton connections\n",
    "                skeleton_connections = [\n",
    "                    (\"left_shoulder\", \"right_shoulder\"), (\"left_shoulder\", \"left_elbow\"), (\"left_elbow\", \"left_wrist\"),\n",
    "                    (\"right_shoulder\", \"right_elbow\"), (\"right_elbow\", \"right_wrist\"),\n",
    "                    (\"left_shoulder\", \"left_hip\"), (\"right_shoulder\", \"right_hip\"),\n",
    "                    (\"left_hip\", \"right_hip\"), (\"left_hip\", \"left_knee\"), (\"left_knee\", \"left_ankle\"),\n",
    "                    (\"right_hip\", \"right_knee\"), (\"right_knee\", \"right_ankle\"),\n",
    "                    (\"nose\", \"left_eye\"), (\"nose\", \"right_eye\"),\n",
    "                    (\"left_eye\", \"left_ear\"), (\"right_eye\", \"right_ear\")\n",
    "                ]\n",
    "                for bp1, bp2 in skeleton_connections:\n",
    "                    if bp1 in keypoints and bp2 in keypoints:\n",
    "                        cv2.line(frame, keypoints[bp1], keypoints[bp2], (255, 255, 255), 2)\n",
    "    \n",
    "        # Draw corrected IDs\n",
    "        for _, box in frame_boxes.iterrows():\n",
    "            pid = int(box.Person_ID)\n",
    "            x_center = int((box.X1 + box.X2) / 2)\n",
    "            y_center = int((box.Y1 + box.Y2) / 2)\n",
    "            color = colors.setdefault(pid, tuple(np.random.randint(0, 255, size=3).tolist()))\n",
    "            cv2.putText(frame, f'ID {pid}', (x_center + 10, y_center - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 5)\n",
    "    \n",
    "        out_video.write(frame)\n",
    "    \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    out_video.release()\n",
    "    print(\"‚úÖ Visual inspection video created!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === FINAL CORRECT MATCHED CSV (with likelihoods) ===\n",
    "\n",
    "final_data = []\n",
    "\n",
    "for idx, row in tqdm(final_output.iterrows(), total=len(final_output), desc=\"Creating Final Output\"):\n",
    "    frame_num = int(row[\"Frame\"])\n",
    "    box_id = int(row[\"Matched_Person_ID\"])\n",
    "    skel_id = row[\"Skeleton_ID\"]\n",
    "\n",
    "    # Prepare one row\n",
    "    data = {\n",
    "        \"Frame\": frame_num,\n",
    "        \"ID\": box_id,\n",
    "        \"X1\": row[\"X1\"],\n",
    "        \"Y1\": row[\"Y1\"],\n",
    "        \"X2\": row[\"X2\"],\n",
    "        \"Y2\": row[\"Y2\"],\n",
    "    }\n",
    "\n",
    "    # Find skeleton number\n",
    "    if isinstance(skel_id, str) and skel_id.startswith(\"idv_\"):\n",
    "        skel_num = skel_id.split(\"_\")[1]\n",
    "    else:\n",
    "        continue  # skip if no skeleton match\n",
    "\n",
    "    # Add bodyparts (x, y, likelihood)\n",
    "    for part in target_bodyparts:\n",
    "        x_col = f\"idv_{skel_num}_{part}_x\"\n",
    "        y_col = f\"idv_{skel_num}_{part}_y\"\n",
    "        conf_col = f\"idv_{skel_num}_{part}_likelihood\"\n",
    "        \n",
    "        data[f\"{part}_x\"] = row.get(x_col, np.nan)\n",
    "        data[f\"{part}_y\"] = row.get(y_col, np.nan)\n",
    "        data[f\"{part}_conf\"] = row.get(conf_col, np.nan)\n",
    "\n",
    "    final_data.append(data)\n",
    "\n",
    "# Create DataFrame\n",
    "final_df = pd.DataFrame(final_data)\n",
    "\n",
    "# Sort nicely\n",
    "final_df = final_df.sort_values([\"Frame\", \"ID\"])\n",
    "\n",
    "# Save\n",
    "final_df.to_csv(OUTPUT_FOLDER / \"final_corrected_flat_output_with_likelihood.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Corrected FINAL CSV saved with x, y, and likelihoods!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplabcut)",
   "language": "python",
   "name": "deeplabcut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
