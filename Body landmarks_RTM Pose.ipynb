{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1be82-fb73-48ed-a226-ebadc49a255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do four steps: \n",
    "# 1: Export bounding boxes with YOLO: \n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.15\n",
    "\n",
    "# === Paths ===\n",
    "video_folder = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO\")\n",
    "output_base = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO/DLC\")\n",
    "\n",
    "if output_base.exists():\n",
    "    shutil.rmtree(output_base)\n",
    "output_base.mkdir(parents=True)\n",
    "\n",
    "video_files = list(video_folder.glob(\"*.mp4\")) + list(video_folder.glob(\"*.MP4\")) + \\\n",
    "              list(video_folder.glob(\"*.Mp4\")) + list(video_folder.glob(\"*.mP4\"))\n",
    "\n",
    "print(f\"ðŸŽ¥ Found {len(video_files)} video(s) to process.\")\n",
    "if not video_files:\n",
    "    raise FileNotFoundError(\"No video files found!\")\n",
    "\n",
    "# === Load YOLOv11 model\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"âš¡ Using device: {device}\")\n",
    "model = YOLO(\"yolo11l.pt\")\n",
    "\n",
    "# === Detection Loop ===\n",
    "for i, video_path in enumerate(tqdm(video_files, desc=\"YOLO Detection\", position=0)):\n",
    "    print(f\"\\nðŸ“¹ [{i+1}/{len(video_files)}] Processing: {video_path.name}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"âš ï¸ Could not open video: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_name = video_path.stem\n",
    "    frame_output_dir = output_base / video_name / \"frames\"\n",
    "    bbox_output_dir = output_base / video_name / \"bboxes\"\n",
    "    frame_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    bbox_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    with tqdm(total=total_frames, desc=\"Processing frames\", leave=False, position=1) as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                break\n",
    "\n",
    "            # Save frame\n",
    "            frame_path = frame_output_dir / f\"{frame_idx:05d}.jpg\"\n",
    "            cv2.imwrite(str(frame_path), frame)\n",
    "\n",
    "            # Run YOLOv8 detection (no classes filter here!)\n",
    "            results = model.predict(frame, verbose=False, device=device, imgsz=640)\n",
    "\n",
    "            # Manually filter for persons (class 0)\n",
    "            bboxes_xywh = []\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None and boxes.xyxy is not None:\n",
    "                    xyxy = boxes.xyxy.cpu().numpy()\n",
    "                    cls = boxes.cls.cpu().numpy()\n",
    "                    conf = boxes.conf.cpu().numpy()  # â† Add this line to get the confidences\n",
    "                    for (x1, y1, x2, y2), label, confidence in zip(xyxy, cls, conf):\n",
    "                        if int(label) == 0 and confidence >= CONFIDENCE_THRESHOLD:  # class 0 = person + confidence check\n",
    "                            x = float(x1)\n",
    "                            y = float(y1)\n",
    "                            w = float(x2 - x1)\n",
    "                            h = float(y2 - y1)\n",
    "                            bboxes_xywh.append([x, y, w, h])\n",
    "\n",
    "            bboxes_xywh = np.array(bboxes_xywh[:20], dtype=np.float32)\n",
    "\n",
    "            if bboxes_xywh.size == 0:\n",
    "                bboxes_xywh = np.empty((0, 4), dtype=np.float32)\n",
    "\n",
    "            bbox_path = bbox_output_dir / f\"{frame_idx:05d}.npy\"\n",
    "            np.save(str(bbox_path), bboxes_xywh)\n",
    "\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"âœ… Finished: {video_name} with {frame_idx} frames processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6c342-1e6e-4592-a1b6-1474ee511efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: RTM pose with DLC:\n",
    "\n",
    "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
    "from deeplabcut.utils.video_processor import VideoProcessorCV\n",
    "from deeplabcut.utils.make_labeled_video import CreateVideo\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "import deeplabcut.utils\n",
    "deeplabcut.utils.tqdm = tqdm\n",
    "import shutil\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    "\n",
    "\n",
    "\n",
    "# === Model Configuration Paths ===\n",
    "path_model_config = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7_pytorch_config.yaml\")\n",
    "path_snapshot = Path(\"/Users/Christian/rtm_pose/rtmpose-x_simcc-body7.pt\")\n",
    "input_folder = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO/DLC\") # Change the folder here!!!\n",
    "\n",
    "# === Pose Model Settings ===\n",
    "device = \"mps\"  # Use Apple Silicon MPS\n",
    "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
    "runner = dlc_torch.get_pose_inference_runner(\n",
    "    pose_cfg,\n",
    "    snapshot_path=path_snapshot,\n",
    "    batch_size=4,\n",
    "    max_individuals=20,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# === Load video directories ===\n",
    "video_dirs = [d for d in input_folder.iterdir() if d.is_dir()]\n",
    "print(f\"ðŸ“‚ Found {len(video_dirs)} videos to process.\")\n",
    "\n",
    "# === Pose Estimation Loop ===\n",
    "for video_dir in tqdm(video_dirs, desc=\"Pose Estimation\", position=0):\n",
    "    print(f\"\\nðŸ§â€â™‚ï¸ Processing: {video_dir.name}\")\n",
    "    frame_dir = video_dir / \"frames\"\n",
    "    bbox_dir = video_dir / \"bboxes\"\n",
    "\n",
    "    frame_files = sorted(frame_dir.glob(\"*.jpg\"))\n",
    "    bbox_files = sorted(bbox_dir.glob(\"*.npy\"))\n",
    "\n",
    "    assert len(frame_files) == len(bbox_files), \"Mismatch between frames and bbox files.\"\n",
    "\n",
    "    output_csv_path = input_folder / f\"{video_dir.name}_predictions.csv\"\n",
    "    partial_predictions = {}\n",
    "\n",
    "    with tqdm(total=len(frame_files), desc=\"Pose estimation frames\", leave=False, position=1) as pbar:\n",
    "        for idx, (frame_file, bbox_file) in enumerate(zip(frame_files, bbox_files)):\n",
    "            frame = cv2.imread(str(frame_file))\n",
    "            if frame is None:\n",
    "                print(f\"âš ï¸ Failed to load frame: {frame_file}\")\n",
    "                continue\n",
    "\n",
    "            bboxes = np.load(str(bbox_file), allow_pickle=True)\n",
    "            frame_context = {\"bboxes\": bboxes}\n",
    "\n",
    "            # Run inference on single frame\n",
    "            pred = runner.inference([(frame, frame_context)])[0]\n",
    "            partial_predictions[idx] = pred\n",
    "\n",
    "            # Save every 100 frames\n",
    "            if (idx + 1) % 100 == 0 or (idx + 1) == len(frame_files):\n",
    "                df_partial = dlc_torch.build_predictions_dataframe(\n",
    "                    scorer=\"rtmpose-body7\",\n",
    "                    predictions=partial_predictions,\n",
    "                    parameters=dlc_torch.PoseDatasetParameters(\n",
    "                        bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                        unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "                        individuals=[f\"idv_{i}\" for i in range(20)]\n",
    "                    )\n",
    "                )\n",
    "                df_partial.to_csv(output_csv_path)\n",
    "                print(f\"ðŸ’¾ Saved intermediate predictions at frame {idx+1}\")\n",
    "        \n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"âœ… Finished pose estimation: {video_dir.name}\")\n",
    "\n",
    "    create_labeled_video = False  # Set this to False if you DON'T want labeled videos!!!\n",
    "\n",
    "    # === Optional: Create labeled video IF NEEDED\n",
    "    if create_labeled_video:\n",
    "        original_video_path = Path(\"XXX\") / f\"{video_dir.name}.mp4\"\n",
    "        output_video_path = input_folder / f\"{video_dir.name}_labeled.mp4\"\n",
    "    \n",
    "        if original_video_path.exists():\n",
    "            clip = VideoProcessorCV(str(original_video_path), sname=str(output_video_path), codec=\"mp4v\")\n",
    "            df_final = dlc_torch.build_predictions_dataframe(\n",
    "                scorer=\"rtmpose-body7\",\n",
    "                predictions=partial_predictions,\n",
    "                parameters=dlc_torch.PoseDatasetParameters(\n",
    "                    bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                    unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "                    individuals=[f\"idv_{i}\" for i in range(20)]\n",
    "                )\n",
    "            )\n",
    "        \n",
    "            print(f\"ðŸŽ¬ Creating labeled video: {output_video_path.name}\", end=\"\", flush=True)\n",
    "            \n",
    "            with suppress_stdout():\n",
    "                CreateVideo(\n",
    "                    clip,\n",
    "                    df_final,\n",
    "                    pcutoff=0.4,\n",
    "                    dotsize=5,\n",
    "                    colormap=\"rainbow\",\n",
    "                    bodyparts2plot=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "                    trailpoints=0,\n",
    "                    cropping=False,\n",
    "                    x1=0,\n",
    "                    x2=clip.w,\n",
    "                    y1=0,\n",
    "                    y2=clip.h,\n",
    "                    bodyparts2connect=[\n",
    "                        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],\n",
    "                        [5, 11], [6, 12], [5, 6], [5, 7], [6, 8],\n",
    "                        [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
    "                        [1, 3], [2, 4], [3, 5], [4, 6]\n",
    "                    ],\n",
    "                    skeleton_color=\"k\",\n",
    "                    draw_skeleton=True,\n",
    "                    displaycropped=False,\n",
    "                    color_by=\"bodypart\",\n",
    "                )\n",
    "            print(f\"ðŸŽ¬ Labeled video saved: {output_video_path.name}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Original video {original_video_path.name} not found, skipping labeled video.\")\n",
    "    \n",
    "    # Continue cleanup regardless of the flag\n",
    "    del partial_predictions\n",
    "    torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nðŸŽ‰ All pose estimations complete!\")\n",
    "\n",
    "# === REMOVE ALL DLC SUBFOLDERS (after all pose estimations are complete) ===\n",
    "for subfolder in input_folder.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(subfolder)\n",
    "            print(f\"ðŸ—‘ï¸ Deleted DLC subfolder: {subfolder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error deleting {subfolder}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390199a7-760b-4c52-827e-929324f379a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Merge skeletons with YOLO Person detection\n",
    "\n",
    "# REASSIGNMENT === UPDATED SCRIPT: Add Full Skeleton ===\n",
    "# === FINAL CLEAN SCRIPT: Stable ID Assignment + Pose Matching + True Reassignment (Skeleton to Skeleton) ===\n",
    "\n",
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === SETTINGS ===\n",
    "MATCH_THRESHOLD_FRAME = 150          # Pixel-based threshold if not normalizing\n",
    "USE_NORMALIZED_DISTANCE = True       # Switch for normalized matching, otherwise fixed distance matching\n",
    "NORMALIZED_MATCH_THRESHOLD = 0.3     # Threshold if normalizing distances\n",
    "\n",
    "# === LOAD VIDEO FILES ===\n",
    "base_folder = Path(\"/Users/Christian/Downloads/Microadaptive Teaching Dritter Teil - LAs/Marlon/Erste Sitzung/YOLO\")\n",
    "video_files = list(base_folder.glob(\"*.mp4\")) + list(base_folder.glob(\"*.MP4\"))\n",
    "\n",
    "# === MAIN LOOP: PROCESS EACH VIDEO ===\n",
    "for video_path in video_files:\n",
    "    print(f\"\\nðŸŽ¥ Processing video: {video_path.name}\")\n",
    "\n",
    "    INPUT_PATH = video_path\n",
    "\n",
    "    # === LOAD TRACKING DATA (Bounding Boxes) ===\n",
    "    tracking_df = pd.read_csv(INPUT_PATH.parent / \"YOLO\" / (INPUT_PATH.stem + \".csv\"))\n",
    "\n",
    "    # === LOAD POSE ESTIMATION DATA (Skeletons) ===\n",
    "    pose_df = pd.read_csv(INPUT_PATH.parent / \"DLC\" / (INPUT_PATH.stem + \"_predictions.csv\"), low_memory=False)\n",
    "\n",
    "    # === PREPARE POSE DATAFRAME ===\n",
    "    multi_index = pd.MultiIndex.from_arrays(pose_df.iloc[0:3].values, names=[\"individual\", \"bodypart\", \"coord\"])\n",
    "    pose_data_cleaned = pose_df.iloc[3:].copy()\n",
    "    pose_data_cleaned.columns = multi_index\n",
    "    pose_data_cleaned.reset_index(drop=True, inplace=True)\n",
    "    pose_data_cleaned[\"Frame\"] = pose_data_cleaned.index.astype(int)\n",
    "\n",
    "    # === DETECT BODY PARTS (skip non-body columns) ===\n",
    "    bodyparts = pose_data_cleaned.columns.get_level_values(\"bodypart\").unique()\n",
    "    bodyparts = [bp for bp in bodyparts if bp not in [\"Frame\", \"scorer\"]]\n",
    "\n",
    "    # === FINAL DATA COLLECTOR ===\n",
    "    final_data = []\n",
    "\n",
    "    # === FRAME-BY-FRAME PROCESSING ===\n",
    "    for frame_idx, pose_row in tqdm(pose_data_cleaned.iterrows(), total=len(pose_data_cleaned), desc=f\"Processing {INPUT_PATH.stem}\"):\n",
    "        frame_num = int(pose_row[\"Frame\"].iloc[0] if isinstance(pose_row[\"Frame\"], pd.Series) else pose_row[\"Frame\"])\n",
    "        frame_boxes = tracking_df[tracking_df.Frame == frame_num]\n",
    "\n",
    "        # === EXTRACT ALL SKELETON CENTERS IN THIS FRAME ===\n",
    "        skeleton_centers = {}\n",
    "        individuals = pose_row.index.get_level_values(\"individual\").unique()\n",
    "        individuals = [ind for ind in individuals if ind.startswith(\"idv_\")]\n",
    "\n",
    "        for ind in individuals:\n",
    "            keypoints = []\n",
    "            for bp in bodyparts:\n",
    "                try:\n",
    "                    x = float(pose_row[(ind, bp, \"x\")])\n",
    "                    y = float(pose_row[(ind, bp, \"y\")])\n",
    "                    if pd.notna(x) and pd.notna(y):\n",
    "                        keypoints.append((x, y))\n",
    "                except:\n",
    "                    continue\n",
    "            if keypoints:\n",
    "                cx, cy = np.mean(keypoints, axis=0)  # Compute mean x, mean y = skeleton center\n",
    "                skeleton_centers[ind] = (cx, cy)\n",
    "\n",
    "        # === MATCH EACH BOUNDING BOX TO NEAREST SKELETON ===\n",
    "        for _, box in frame_boxes.iterrows():\n",
    "            x_center = (box.X1 + box.X2) / 2\n",
    "            y_center = (box.Y1 + box.Y2) / 2\n",
    "            pid = int(box.Person_ID)\n",
    "\n",
    "            best_match = None\n",
    "            min_distance = float('inf')\n",
    "\n",
    "            for skel_id, (cx, cy) in skeleton_centers.items():\n",
    "                center_distance = np.linalg.norm(np.array([cx, cy]) - np.array([x_center, y_center]))\n",
    "\n",
    "                # === OPTIONAL: Normalize distance by bounding box size ===\n",
    "                if USE_NORMALIZED_DISTANCE:\n",
    "                    bbox_width = box.X2 - box.X1\n",
    "                    bbox_height = box.Y2 - box.Y1\n",
    "                    avg_bbox_size = (bbox_width + bbox_height) / 2\n",
    "                    if avg_bbox_size > 0:\n",
    "                        center_distance /= avg_bbox_size\n",
    "\n",
    "                threshold = NORMALIZED_MATCH_THRESHOLD if USE_NORMALIZED_DISTANCE else MATCH_THRESHOLD_FRAME\n",
    "\n",
    "                # === KEEP CLOSEST SKELETON UNDER THRESHOLD ===\n",
    "                if center_distance < min_distance and center_distance < threshold:\n",
    "                    best_match = skel_id\n",
    "                    min_distance = center_distance\n",
    "\n",
    "            # === SAVE MATCH INFORMATION ===\n",
    "            data = {\n",
    "                \"Frame\": frame_num,\n",
    "                \"Old_ID\": pid,\n",
    "                \"New_ID\": pid,\n",
    "                \"X1\": box.X1,\n",
    "                \"Y1\": box.Y1,\n",
    "                \"X2\": box.X2,\n",
    "                \"Y2\": box.Y2,\n",
    "            }\n",
    "\n",
    "            if best_match:\n",
    "                skel_num = best_match.split(\"_\")[1]\n",
    "                for bp in bodyparts:\n",
    "                    x = pose_row.get((f\"idv_{skel_num}\", bp, \"x\"), np.nan)\n",
    "                    y = pose_row.get((f\"idv_{skel_num}\", bp, \"y\"), np.nan)\n",
    "                    conf = pose_row.get((f\"idv_{skel_num}\", bp, \"likelihood\"), np.nan)\n",
    "                    data[f\"{bp}_x\"] = x\n",
    "                    data[f\"{bp}_y\"] = y\n",
    "                    data[f\"{bp}_conf\"] = conf\n",
    "\n",
    "            final_data.append(data)\n",
    "\n",
    "    # === SAVE FINAL CORRECTED CSV ===\n",
    "    final_df = pd.DataFrame(final_data)\n",
    "    final_df = final_df.sort_values([\"Frame\", \"New_ID\"])\n",
    "    final_df.drop(columns=[\"bodyparts_x\", \"bodyparts_y\", \"bodyparts_conf\", \"_x\", \"_y\", \"_conf\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    OUTPUT_FOLDER = INPUT_PATH.parent / \"corrected\"\n",
    "    OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_csv_name = f\"{INPUT_PATH.stem}_corrected.csv\"\n",
    "    final_df.to_csv(OUTPUT_FOLDER / output_csv_name, index=False)\n",
    "\n",
    "    print(\"\\u2705 Final corrected CSV saved for:\", video_path.name)\n",
    "\n",
    "print(\"\\nðŸŽ‰ All sessions processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b11956-ee9e-473d-83f5-e8855e1d85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Merge IDs, skeletons and object detection: \n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === PATHS ===\n",
    "main_folder = Path(\"/Users/Christian/Downloads/Javelin training/analysis/corrected\")\n",
    "object_folder = Path(\"/Users/Christian/Downloads/Javelin training/analysis/YOLO_Object\")\n",
    "merged_folder = main_folder / \"with_objects\"\n",
    "merged_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === PROCESSING LOOP ===\n",
    "for corrected_file in main_folder.glob(\"*_corrected.csv\"):\n",
    "    base_name = corrected_file.stem.replace(\"_corrected\", \"\")\n",
    "    object_file = object_folder / f\"{base_name}.csv\"\n",
    "\n",
    "    if not object_file.exists():\n",
    "        print(f\"Skipping {base_name} â€” no object file found.\")\n",
    "        continue\n",
    "\n",
    "    # Load both dataframes\n",
    "    df_people = pd.read_csv(corrected_file)\n",
    "    df_object = pd.read_csv(object_file)\n",
    "\n",
    "    # Compute center points\n",
    "    df_object[\"center_x\"] = (df_object[\"X1\"] + df_object[\"X2\"]) / 2\n",
    "    df_object[\"center_y\"] = (df_object[\"Y1\"] + df_object[\"Y2\"]) / 2\n",
    "\n",
    "    # Pivot: one row per frame, columns like Tip_center_x, Handle_center_y, etc.\n",
    "    # Keep only needed columns\n",
    "    df_object_slim = df_object[[\"Frame\", \"Label\", \"X1\", \"Y1\", \"X2\", \"Y2\", \"center_x\", \"center_y\"]]\n",
    "    \n",
    "    # Pivot: one row per frame, columns like Tip_X1, Tip_center_x, etc.\n",
    "    df_object_pivot = df_object_slim.pivot_table(\n",
    "        index=\"Frame\",\n",
    "        columns=\"Label\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Flatten multi-index columns\n",
    "    df_object_pivot.columns = [f\"{label}_{coord}\" for coord, label in df_object_pivot.columns]\n",
    "    df_object_pivot.reset_index(inplace=True)\n",
    "\n",
    "    # Merge person + object center info\n",
    "    df_merged = df_people.merge(df_object_pivot, on=\"Frame\", how=\"left\")\n",
    "\n",
    "    # Save to new folder\n",
    "    out_path = merged_folder / f\"{base_name}_merged.csv\"\n",
    "    df_merged.to_csv(out_path, index=False)\n",
    "    print(f\"âœ… Merged and saved: {out_path.name}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All sessions processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af43e72-4e54-4f54-bbb5-a36de096ded3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplabcut)",
   "language": "python",
   "name": "deeplabcut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
